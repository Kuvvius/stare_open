{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4a511-5879-4af0-9441-359d6fd02740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f1(gts, preds):\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    f1_macro = f1_score(gts, preds, average='macro')\n",
    "    acc = np.sum(np.array(gts) == np.array(preds)) / len(gts)\n",
    "    return \"acc\", acc, \"f1 macro\", f1_macro\n",
    "def extract_letter(text):\n",
    "    import re\n",
    "    # Regular expression to capture a single letter (A, B, C, D) inside \\boxed{}\n",
    "    match = re.search(r'\\\\boxed\\{.*?\\b([A-D])\\b.*?\\}', text)\n",
    "    \n",
    "    return match.group(1) if match else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43342300-d9cc-436a-a0ca-24f9b9237769",
   "metadata": {},
   "source": [
    "## Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd1b25-69ee-45a2-ac77-249a8d9d85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import io\n",
    "import base64\n",
    "client = OpenAI()\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def pil_to_base64(pil_image):\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_encoded_str = base64.b64encode(img_byte_arr.getvalue()).decode('ascii')\n",
    "    return img_encoded_str\n",
    "\n",
    "\n",
    "def extract_answer_from_model_response(model_response):\n",
    "    model_answer = model_response.strip()\n",
    "    model_answer = model_answer.replace(\"option\", \"\")\n",
    "    try:\n",
    "        model_answer = model_answer.split('oxed{')[-1][0]\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return model_answer\n",
    "\n",
    "\n",
    "answer_dict = {}\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"MahtabBg/NEWPerspective\")\n",
    "dataset = dataset['train']\n",
    "if True:\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        example = dataset[i]\n",
    "        qid = example['id']\n",
    "\n",
    "        question_image = example['topdown']\n",
    "        choice_image = example['choices']\n",
    "        question_image = pil_to_base64(question_image)\n",
    "        choice_image = pil_to_base64(choice_image)\n",
    "        \n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 1>:\",\n",
    "                        },\n",
    "\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\":f\"data:image/jpeg;base64,{question_image}\", \"detail\": \"auto\",\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 2>:\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{choice_image}\", \"detail\": \"auto\",\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 1> shows an image from the top of a scene with a red square indicating an agent and a red arrow indicating the agent's direction of view.\\nSelect from the <Image 2> which image represents the agent's view. Please first solve the problem step by step, then put your final answer or a single letter (if it is a multiple choice question) in one \\\"\\\\boxed{}\\\"\",\n",
    "                        },\n",
    "                        \n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "            # temperature=0,\n",
    "        )\n",
    "        \n",
    "        answer_dict[qid] = {\"response\":response.choices[0].message.content , \"pred\": extract_answer_from_model_response(response.choices[0].message.content)}\n",
    "        if len(answer_dict)%10==0:\n",
    "            np.save(\"perspective_gpt4o.npy\", answer_dict)\n",
    "np.save(\"perspective_gpt4o.npy\", answer_dict)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7540c985-346d-4bc7-ae3b-0af1bccb57ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dict= np.load(\"perspective_gpt4o.npy\" ,allow_pickle=True).item()\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"MahtabBg/NEWPerspective\")\n",
    "dataset = dataset['train']\n",
    "map_dict = {\"up\": \"A\", \"right\": \"B\", \"down\": \"C\", \"left\": \"D\"}\n",
    "gts, preds =[], []\n",
    "for i in range(len(dataset)):\n",
    "    el = dataset[i]\n",
    "    gts.append(map_dict[el['answer']].lower())\n",
    "    pred = extract_letter(answer_dict[el['id']]['response']).lower()\n",
    "    assert pred in ['a', 'b', 'c', 'd']\n",
    "    # if pred!= answer_dict[el['id']]['pred'].lower():\n",
    "    #     print(pred, answer_dict[el['id']]['pred'].lower(), answer_dict[el['id']]['response'])\n",
    "    preds.append(pred)\n",
    "\n",
    "calc_f1(gts, preds)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6a15c-df83-4554-ba04-52df438d44f1",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e866bc-f7ca-4ea6-b2e9-bb101fa70f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "import base64\n",
    "client = OpenAI()\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        \n",
    "def pil_to_base64(pil_image):\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_encoded_str = base64.b64encode(img_byte_arr.getvalue()).decode('ascii')\n",
    "    return img_encoded_str\n",
    "    \n",
    "def extract_answer_from_model_response(model_response):\n",
    "    model_answer = model_response.strip()\n",
    "    model_answer = model_answer.replace(\"option\", \"\")\n",
    "    try:\n",
    "        model_answer = model_answer.split('oxed{')[-1][0]\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return model_answer\n",
    "\n",
    "\n",
    "dataset = load_dataset(f\"MahtabBg/Video\")\n",
    "dataset = dataset['train']\n",
    "\n",
    "\n",
    "answer_dict = {}\n",
    "from datasets import load_dataset\n",
    "\n",
    "if True:\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        example = dataset[i]\n",
    "        qid = example['id']\n",
    "\n",
    "        \n",
    "       \n",
    "        question_image = example['question']\n",
    "        choice_image = example['choices']\n",
    "        \n",
    "        question_image = pil_to_base64(question_image)\n",
    "        choice_image = pil_to_base64(choice_image)\n",
    "        \n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                         {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 1>:\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\":f\"data:image/jpeg;base64,{question_image}\", \"detail\": \"auto\",\n",
    "                            },\n",
    "                        },\n",
    "                         {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 2>:\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{choice_image}\", \"detail\": \"auto\",\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"You see 4 sequential frames of a video in <Image 1>, but one is missing (marked with '?').\\n Choose which of the images in <Image 2> correctly fills the missing frame.\\nRemember, the camera only moves in one direction (left or right) in the video. Please first solve the problem step by step, then put your final answer or a single letter (if it is a multiple choice question) in one \\\"\\\\boxed{}\\\".\",\n",
    "                        },\n",
    "                        \n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "            # temperature=0,\n",
    "        )\n",
    "       \n",
    "        \n",
    "        answer_dict[qid] = {\"response\":response.choices[0].message.content , \"pred\": extract_answer_from_model_response(response.choices[0].message.content)}\n",
    "        if len(answer_dict)%10==0:\n",
    "            np.save(\"video_gpt4o.npy\", answer_dict)\n",
    "np.save(\"video_gpt4o.npy\", answer_dict)     \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc5dbe-3676-4ae6-b874-30baf9b7ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dict= np.load(\"video_gpt4o.npy\" ,allow_pickle=True).item()\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"MahtabBg/Video\")\n",
    "dataset = dataset['train']\n",
    "gts, preds =[], []\n",
    "for i in range(len(dataset)):\n",
    "    el = dataset[i]\n",
    "    gts.append(el['answer'].lower())\n",
    "    pred = extract_letter(answer_dict[el['id']]['response']).lower()\n",
    "    assert pred in ['a', 'b', 'c']\n",
    "    # if pred!= answer_dict[el['id']]['pred'].lower():\n",
    "    #     print(pred, answer_dict[el['id']]['pred'].lower(), answer_dict[el['id']]['response'])\n",
    "    preds.append(pred)\n",
    "\n",
    "calc_f1(gts, preds) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
