{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88960d9b-95d0-4adc-b514-0230b1fa1d90",
   "metadata": {},
   "source": [
    "# Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d74abd5-585f-498b-b950-d8357eed712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import numpy as np\n",
    "# load API key\n",
    "GEMINI_API_KEY = \"AIzaSyCxuedQMQWQkYSu67h3L5PMXDg3cDmeeBQ\"\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def gemini_call_single(query, model):\n",
    "    # import cv2\n",
    "    import requests\n",
    "    import time\n",
    "    import json\n",
    "    temp=0\n",
    "    # print(query)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            content = client.models.generate_content(\n",
    "                model=model,\n",
    "                contents=query,\n",
    "                # config = types.GenerateContentConfig(\n",
    "                #     temperature=temp,\n",
    "                # )\n",
    "            )\n",
    "            \n",
    "\n",
    "        except Exception as e_msg:\n",
    "            content = '[ERROR] ' + str(e_msg)\n",
    " \n",
    "        if isinstance(content, str):\n",
    "            content = '[ERROR] ' + content.lower()\n",
    "            if 'exceeded call rate limit' in content or 'exhausted' in content:\n",
    "                # retry for unacceptable response\n",
    "                print('\\n(retry later in 5 seconds...) ->', content)\n",
    "                if \"thinking\" in model:\n",
    "                    time.sleep(10)\n",
    "                else:\n",
    "                    time.sleep(5)\n",
    "                continue\n",
    "            else:\n",
    "                print('\\n(retry later...) ->', content)\n",
    "        elif content.text is None:\n",
    "            temp += 0.1\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    \n",
    "    ########################################\n",
    "    \n",
    "    # print(responseJson[\"choices\"][0][\"message\"][\"content\"])\n",
    "    return content.text\n",
    "\n",
    "\n",
    "def extract_answer_from_model_response(model_response):\n",
    "    import re\n",
    "    match = re.search(r'\\\\boxed\\{.*?\\b([A-D]|yes|no)\\b.*?\\}', model_response)\n",
    "    \n",
    "    return match.group(1) if match else \"Z\"\n",
    "\n",
    "\n",
    "# convert PIL image to base64\n",
    "def pil_to_base64(pil_image):\n",
    "    import io\n",
    "    import base64\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_encoded_str = base64.b64encode(img_byte_arr.getvalue()).decode('ascii')\n",
    "    return img_encoded_str\n",
    "\n",
    "\n",
    "\n",
    "def test_gemini_on_VisSim_va(dataset_name, output_dir, model, index, max_tokens=2048, debug=False):\n",
    "\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(f\"VisSim/{dataset_name}\")\n",
    "    dataset = dataset['train']\n",
    "    d_index = list(range(len(dataset)))\n",
    "\n",
    "    \n",
    "    output_path = f\"./{model}_{dataset_name}_{index}.json\"\n",
    "    from collections import defaultdict\n",
    "    acc_by_type = defaultdict(float)\n",
    "    acc_by_difficulty = defaultdict(float)\n",
    "    counts_by_difficulty = defaultdict(int)\n",
    "    counts_by_type = defaultdict(int)\n",
    "\n",
    "    # create output dir\n",
    "    answer_dict= {}\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(d_index, total=len(d_index)):\n",
    "        example = dataset[i]\n",
    "        qid = example['qid']\n",
    "        difficulty_level = example['difficulty_level']\n",
    "        variant = \" \".join(qid.split(\"_\")[1:-1])\n",
    "        # 'Observe the transformation pattern of Shape A through steps 0 to 1. <question_image> Apply the same transformation sequence to Shape B and determine the final shape at step 3. <image_for_B> For reference, the black dots in each panel of the figures indicate the origin. Select the correct answer choice that matches the expected transformation result. <answer_choices>'\n",
    "        A_image = example['A_image']\n",
    "        B_image = example['B_image']\n",
    "        question_info = json.loads(example['question_info'])\n",
    "        question = question_info['question']\n",
    "        choice_image = example['choices']\n",
    "        query = []\n",
    "        prefix, question = question.strip().split(\"<question_image>\")\n",
    "        query.append(prefix)\n",
    "        query.append(A_image)\n",
    "        prefix, question = question.split(\"<image_for_B>\")\n",
    "        query.append(prefix)\n",
    "        query.append(B_image)\n",
    "        prefix, question = question.split(\"<answer_choices>\")   \n",
    "        query.append(prefix)\n",
    "        query.append(choice_image)\n",
    "        if len(question) > 0:\n",
    "            query.append(question)\n",
    "        query.append(\"Please first solve the problem step by step, then put your final answer or a single letter (if it is a multiple choice question) in one \\\"\\\\boxed{}\\\"\")\n",
    "        # print(query)\n",
    "        response = gemini_call_single(query, model=model)\n",
    "\n",
    "        pred = extract_answer_from_model_response(response)\n",
    "\n",
    "        gt_ans = example['answer']\n",
    "\n",
    "        answer_dict[qid] ={\n",
    "            \"pred\": pred,\n",
    "            \"gt_ans\": gt_ans.lower(),\n",
    "            \"response\": response\n",
    "        }\n",
    "            \n",
    "        if \"ans\" not in variant:\n",
    "            acc_by_difficulty[difficulty_level]+= pred.lower() == gt_ans.lower()\n",
    "            counts_by_difficulty[difficulty_level] += 1\n",
    "        acc_by_type[variant] += pred.lower() == gt_ans.lower()\n",
    "        counts_by_type[variant] += 1\n",
    "\n",
    "        if len(answer_dict)%10==0:\n",
    "            np.save(output_path, answer_dict)\n",
    "\n",
    "    # print accuracy\n",
    "    print(dataset, index)\n",
    "    print(\"Accuracy by difficulty level:\")\n",
    "    for k, v in acc_by_difficulty.items():\n",
    "        print(f\"{k}: {v/counts_by_difficulty[k]}\")\n",
    "\n",
    "    print(\"Accuracy by variants:\")\n",
    "    for k, v in acc_by_type.items():\n",
    "        print(f\"{k}: {v/counts_by_type[k]}\")\n",
    "    \n",
    "    overall_acc = sum(acc_by_difficulty.values())/sum(counts_by_difficulty.values())\n",
    "    print(f\"Overall accuracy: {overall_acc}\")\n",
    "    np.save(output_path, answer_dict)\n",
    "\n",
    "def test_gemini_on_VisSim_text_inst(dataset_name, output_dir, model, index, max_tokens=2048,debug=False):\n",
    "\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(f\"VisSim/{dataset_name}\")\n",
    "    dataset = dataset['train']\n",
    "    d_index = list(range(len(dataset)))\n",
    "   \n",
    "\n",
    "\n",
    "    from collections import defaultdict\n",
    "    acc_by_type = defaultdict(float)\n",
    "    acc_by_difficulty = defaultdict(float)\n",
    "    counts_by_difficulty = defaultdict(int)\n",
    "    counts_by_type = defaultdict(int)\n",
    "\n",
    "    output_path  = f\"./{model}_{dataset_name}_{index}.json\"\n",
    "    if os.path.exists(output_path+\".npy\"):\n",
    "        answer_dict = np.load(output_path+\".npy\", allow_pickle=True).item()\n",
    "    else:\n",
    "        answer_dict={}\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(d_index, total=len(d_index)):\n",
    "        example = dataset[i]\n",
    "        qid = example['qid']\n",
    "        if qid in answer_dict:\n",
    "            continue\n",
    "        difficulty_level = example['difficulty_level']\n",
    "        variant = \" \".join(qid.split(\"_\")[1:-1])\n",
    "        \n",
    "        images = example['images'][:-1]\n",
    "        # question_info = json.loads(example['question_info'])\n",
    "        question = example['question']\n",
    "        choice_image = example['choices']\n",
    "\n",
    "        # use regex to parse the question and place the images in the right spots\n",
    "        query = []\n",
    "        for i, image in enumerate(images):\n",
    "            if i == 0:\n",
    "                prefix, question = question.strip().split(\"<shapeB_image>\")\n",
    "            else:\n",
    "                prefix, question = question.split(f\"<shapeB_step_{i-1}>\")\n",
    "            query.append(prefix)\n",
    "            query.append(image)\n",
    "        \n",
    "        # replace the remaining <shapeB_image> with \"\" using regex\n",
    "        import re\n",
    "        # using wildcards to match the <shapeB_step_{i}> and replace it with \"\"\n",
    "        query.append(re.sub(r'<shapeB_step_\\d+>', '', question))\n",
    "\n",
    "        query.append(choice_image)\n",
    "        \n",
    "        query.append(\"Please first solve the problem step by step, then put your final answer or a single letter (if it is a multiple choice question) in one \\\"\\\\boxed{}\\\"\")\n",
    "        # print(query)\n",
    "        response = gemini_call_single(query, model=model)\n",
    "\n",
    "        pred = extract_answer_from_model_response(response)\n",
    "\n",
    "        gt_ans = example['answer']\n",
    "\n",
    "        answer_dict[qid]={\n",
    "            \"pred\": pred,\n",
    "            \"gt_ans\": gt_ans.lower(),\n",
    "            \"response\": response\n",
    "        }\n",
    "        if len(answer_dict)%10==0:\n",
    "            np.save(output_path, answer_dict)\n",
    "        if \"ans\" not in variant:\n",
    "            acc_by_difficulty[difficulty_level]+= pred.lower() == gt_ans.lower()\n",
    "            counts_by_difficulty[difficulty_level] += 1\n",
    "        acc_by_type[variant] += pred.lower() == gt_ans.lower()\n",
    "        counts_by_type[variant] += 1\n",
    "\n",
    "    # print accuracy\n",
    "    np.save(output_path, answer_dict)\n",
    "    print(\"Accuracy by difficulty level:\")\n",
    "    for k, v in acc_by_difficulty.items():\n",
    "        print(f\"{k}: {v/counts_by_difficulty[k]}\")\n",
    "\n",
    "    print(\"Accuracy by variants:\")\n",
    "    for k, v in acc_by_type.items():\n",
    "        print(f\"{k}: {v/counts_by_type[k]}\")\n",
    "    \n",
    "    overall_acc = sum(acc_by_difficulty.values())/sum(counts_by_difficulty.values())\n",
    "    print(f\"Overall accuracy: {overall_acc}\")\n",
    "\n",
    "\n",
    "\n",
    "def test_gemini_on_folding_nets(dataset_name, output_dir, model, index, max_tokens=2048,debug=False):\n",
    "\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(f\"VisSim/{dataset_name}\")\n",
    "    dataset = dataset['train']\n",
    "    d_index = list(range(len(dataset)))\n",
    "\n",
    "\n",
    "    from collections import defaultdict\n",
    "    pred_by_type = defaultdict(list)\n",
    "    pred_by_difficulty = defaultdict(list)\n",
    "\n",
    "    gt_by_type = defaultdict(list)\n",
    "    gt_by_difficulty = defaultdict(list)\n",
    "\n",
    "    \n",
    "    output_path  = f\"./{model}_{dataset_name}_{index}.json\"\n",
    "    if os.path.exists(output_path+\".npy\"):\n",
    "        answer_dict = np.load(output_path+\".npy\", allow_pickle=True).item()\n",
    "    else:\n",
    "        answer_dict={}\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(d_index, total=len(d_index)):\n",
    "        example = dataset[i]\n",
    "        qid = example['qid']\n",
    "        if qid in answer_dict:\n",
    "            continue\n",
    "        variant = example['type']\n",
    "  \n",
    "        images = example['images']\n",
    "        # question_info = json.loads(example['question_info'])\n",
    "        question = example['question']\n",
    "\n",
    "        # use regex to parse the question and place the images in the right spots\n",
    "        query = []\n",
    "        for i, image in enumerate(images):\n",
    "            prefix, question = question.split(f\"<image_{i}>\")\n",
    "            query.append(prefix)\n",
    "            query.append(image)\n",
    "        if len(question) > 0:\n",
    "            query.append(question + \"Think step-by-step, and then put your final answer in \\\"\\\\boxed{}\\\".\")\n",
    "        else:\n",
    "            query.append(\"Think step-by-step, and then put your final answer in \\\"\\\\boxed{}\\\".\")\n",
    "        \n",
    "        # check if the query has at least 1 image after parsing\n",
    "\n",
    "        # print(query)\n",
    "        response = gemini_call_single(query, model=model)\n",
    "\n",
    "        pred = extract_answer_from_model_response(response.lower())\n",
    "\n",
    "        gt_choice = example['answer'].lower()\n",
    "        answer_choices = example['choices']\n",
    "\n",
    "        gt_ans = answer_choices[int(ord(gt_choice) - ord('a'))]\n",
    "\n",
    "        answer_dict[ qid]={\n",
    "            \"pred\": pred,\n",
    "            \"gt_choice\": gt_choice,\n",
    "            \"gt_ans\": gt_ans.lower(),\n",
    "            \"response\": response,\n",
    "            \"question\": [q  if isinstance(q, str) else '<image>' for q in query]\n",
    "        }\n",
    "        if len(answer_dict)%10==0:\n",
    "            np.save(output_path, answer_dict)\n",
    "        correct = pred.lower() == gt_ans.lower() or pred.lower() == gt_choice.lower()\n",
    "        pred_by_type[variant].append(pred.lower())\n",
    "        gt_by_type[variant].append(gt_ans.lower())\n",
    "\n",
    "    np.save(output_path, answer_dict)\n",
    "    print(\"F1 by variants:\")\n",
    "    for k in pred_by_type.keys():\n",
    "        from sklearn.metrics import f1_score\n",
    "        print(f\"{k}: {f1_score(gt_by_type[k], pred_by_type[k], average='weighted')}\")\n",
    "\n",
    "    print(\"Random Chance F1 by variants:\")\n",
    "    for k in pred_by_type.keys():\n",
    "        from sklearn.metrics import f1_score\n",
    "        import random\n",
    "        random_pred = [random.choice([\"yes\", \"no\"]) for _ in range(len(gt_by_type[k]))]\n",
    "        print(f\"{k}: {f1_score(gt_by_type[k],random_pred, average='weighted')}\")\n",
    "\n",
    "\n",
    "model = 'gemini-2.0-flash-thinking-exp-01-21'\n",
    "\n",
    "\n",
    "def test_gemini_flash_thinking():\n",
    "    model = 'gemini-2.0-flash-thinking-exp-01-21'\n",
    "    # test_gemini_on_folding_nets('folding_nets_test', 'output_dir/gemini_response', model=model, debug=False)\n",
    "    # test_gemini_on_folding_nets('tangram_puzzle_test', 'output_dir/gemini_response', model=model, debug=False)\n",
    "    # test_gemini_on_VisSim_va('2d_va_test', '.', model=model, index=0, debug=False)\n",
    "    test_gemini_on_VisSim_va('2d_va_test', '.', model=model, index=1, debug=False)\n",
    "    test_gemini_on_VisSim_va('2d_va_test', '.', model=model, index=2, debug=False)\n",
    "    # test_gemini_on_VisSim_text_inst('2d_text_instruct_test', 'output_dir/gemini_response', model=model, debug=False)\n",
    "    # test_gemini_on_folding_nets('folding_nets_vissim_test', 'output_dir/gemini_response', model=model, debug=debug)\n",
    "    # test_gemini_on_folding_nets('tangram_puzzle_vissim_test', 'output_dir/gemini_response', model=model, debug=debug)\n",
    "    # test_gemini_on_VisSim_va('2d_va_vissim_test', 'output_dir/gemini_response', model=model, debug=debug)\n",
    "    # test_gemini_on_VisSim_text_inst('2d_text_instruct_vissim_test', 'output_dir/gemini_response', model=model, debug=debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bf4aea-ce56-4222-a345-1081b19cc461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80511451-57fc-4d5b-85ca-9e4dd8f83751",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gemini_on_VisSim_va('2d_va_test', '.', model=model, index=0, debug=False)\n",
    "test_gemini_on_VisSim_va('2d_va_test', '.', model=model, index=1, debug=False)\n",
    "test_gemini_on_VisSim_va('2d_va_test', '.', model=model, index=2, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bfbebf5-0a84-4ebe-929d-b36bdf12c043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306/306 [00:09<00:00, 30.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['qid', 'A_image', 'B_image', 'choices', 'answer', 'transformations', 'difficulty_level', 'question_info', 'answer_info'],\n",
      "    num_rows: 306\n",
      "}) 0\n",
      "Accuracy by difficulty level:\n",
      "easy: 0.6274509803921569\n",
      "medium: 0.5392156862745098\n",
      "hard: 0.46078431372549017\n",
      "Accuracy by variants:\n",
      "easy no: 0.6274509803921569\n",
      "medium no: 0.5392156862745098\n",
      "hard no: 0.46078431372549017\n",
      "Overall accuracy: 0.5424836601307189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = 'gemini-2.0-flash-thinking-exp-01-21'\n",
    "dataset_name = '2d_va_test'\n",
    "index= 0\n",
    "import numpy as np\n",
    "answer_dict = np.load(\"gemini-2.0-flash-thinking-exp-01-21_2d_va_test_2.json.npy\", allow_pickle=True).item() \n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"VisSim/{dataset_name}\")\n",
    "dataset = dataset['train']\n",
    "d_index = list(range(len(dataset)))\n",
    "\n",
    "\n",
    "output_path = f\"./{model}_{dataset_name}_{index}.json\"\n",
    "from collections import defaultdict\n",
    "acc_by_type = defaultdict(float)\n",
    "acc_by_difficulty = defaultdict(float)\n",
    "counts_by_difficulty = defaultdict(int)\n",
    "counts_by_type = defaultdict(int)\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(d_index, total=len(d_index)):\n",
    "    example = dataset[i]\n",
    "    qid = example['qid']\n",
    "    difficulty_level = example['difficulty_level']\n",
    "    gt = example['answer']\n",
    "    pred = answer_dict[qid]['pred']\n",
    "    variant = \" \".join(qid.split(\"_\")[1:-1])\n",
    "    gt_ans = example['answer']\n",
    "    if pred==\"Z\":\n",
    "        print(answer_dict[qid]['response'])\n",
    "    if \"ans\" not in variant:\n",
    "        acc_by_difficulty[difficulty_level]+= pred.lower() == gt_ans.lower()\n",
    "        counts_by_difficulty[difficulty_level] += 1\n",
    "    acc_by_type[variant] += pred.lower() == gt_ans.lower()\n",
    "    counts_by_type[variant] += 1\n",
    "\n",
    "    # print accuracy\n",
    "print(dataset, index)\n",
    "print(\"Accuracy by difficulty level:\")\n",
    "for k, v in acc_by_difficulty.items():\n",
    "    print(f\"{k}: {v/counts_by_difficulty[k]}\")\n",
    "\n",
    "print(\"Accuracy by variants:\")\n",
    "for k, v in acc_by_type.items():\n",
    "    print(f\"{k}: {v/counts_by_type[k]}\")\n",
    "\n",
    "overall_acc = sum(acc_by_difficulty.values())/sum(counts_by_difficulty.values())\n",
    "print(f\"Overall accuracy: {overall_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e006afea-12b3-48de-823f-c3e66ad77bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 193/193 [00:00<00:00, 221.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 by variants:\n",
      "Random Chance F1 by variants:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 193/193 [58:39<00:00, 18.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 by variants:\n",
      "q_only: 0.5279663665028548\n",
      "q+steps: 0.5031020439234194\n",
      "Random Chance F1 by variants:\n",
      "q_only: 0.4840389962341182\n",
      "q+steps: 0.5893628865913032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 193/193 [1:03:04<00:00, 19.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 by variants:\n",
      "q_only: 0.4334574942769565\n",
      "q+steps: 0.4535728811161606\n",
      "Random Chance F1 by variants:\n",
      "q_only: 0.5332706974687741\n",
      "q+steps: 0.38185920030006676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_gemini_on_folding_nets('folding_nets_test', '.', model=model, index=0, debug=False)\n",
    "test_gemini_on_folding_nets('folding_nets_test', '.', model=model, index=1, debug=False)\n",
    "test_gemini_on_folding_nets('folding_nets_test', '.', model=model, index=2, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e5cc57d-abc3-47f3-b843-54d4a891d810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 193/193 [00:00<00:00, 222.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 by variants:\n",
      "q_only: 0.4334574942769565\n",
      "q+steps: 0.4535728811161606\n",
      "Random Chance F1 by variants:\n",
      "q_only: 0.4583333333333333\n",
      "q+steps: 0.5953698795220393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = 'gemini-2.0-flash-thinking-exp-01-21'\n",
    "dataset_name = 'folding_nets_test'\n",
    "index= 0\n",
    "import numpy as np\n",
    "answer_dict = np.load(\"gemini-2.0-flash-thinking-exp-01-21_folding_nets_test_2.json.npy\", allow_pickle=True).item() \n",
    "print(len(answer_dict))\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"VisSim/{dataset_name}\")\n",
    "dataset = dataset['train']\n",
    "d_index = list(range(len(dataset)))\n",
    "\n",
    "from collections import defaultdict\n",
    "pred_by_type = defaultdict(list)\n",
    "pred_by_difficulty = defaultdict(list)\n",
    "\n",
    "gt_by_type = defaultdict(list)\n",
    "gt_by_difficulty = defaultdict(list)\n",
    "\n",
    "output_path = output_path = f\"./{model}_{dataset_name}_{index}.json\"\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(d_index, total=len(d_index)):\n",
    "    example = dataset[i]\n",
    "    qid = example['qid']\n",
    "    variant = example['type']\n",
    "\n",
    "    \n",
    "    pred = answer_dict[qid]['pred']\n",
    "    if pred == \"Z\":\n",
    "        print(qid, pred)\n",
    "\n",
    "    gt_choice = example['answer'].lower()\n",
    "    answer_choices = example['choices']\n",
    "\n",
    "    gt_ans = answer_choices[int(ord(gt_choice) - ord('a'))]\n",
    "\n",
    "    ect = pred.lower() == gt_ans.lower() or pred.lower() == gt_choice.lower()\n",
    "    pred_by_type[variant].append(pred.lower())\n",
    "    gt_by_type[variant].append(gt_ans.lower())\n",
    "\n",
    "print(\"F1 by variants:\")\n",
    "for k in pred_by_type.keys():\n",
    "    from sklearn.metrics import f1_score\n",
    "    print(f\"{k}: {f1_score(gt_by_type[k], pred_by_type[k], average='weighted')}\")\n",
    "\n",
    "print(\"Random Chance F1 by variants:\")\n",
    "for k in pred_by_type.keys():\n",
    "    from sklearn.metrics import f1_score\n",
    "    import random\n",
    "    random_pred = [random.choice([\"yes\", \"no\"]) for _ in range(len(gt_by_type[k]))]\n",
    "    print(f\"{k}: {f1_score(gt_by_type[k],random_pred, average='weighted')}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c99983-c31e-4a29-a410-f533b44ee3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████▉                                                                                                           | 14/318 [02:48<40:33,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(retry later in 5 seconds...) -> [ERROR] [error] 429 resource_exhausted. {'error': {'code': 429, 'message': 'resource has been exhausted (e.g. check quota).', 'status': 'resource_exhausted'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 318/318 [35:19<00:00,  6.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by difficulty level:\n",
      "easy: 0.6964285714285714\n",
      "medium: 0.7758620689655172\n",
      "hard: 0.7169811320754716\n",
      "Accuracy by variants:\n",
      "no: 0.7305389221556886\n",
      "Overall accuracy: 0.7305389221556886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|██████████████████████████████████▌                                                                             | 98/318 [19:08<27:16,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(retry later in 5 seconds...) -> [ERROR] [error] 429 resource_exhausted. {'error': {'code': 429, 'message': 'resource has been exhausted (e.g. check quota).', 'status': 'resource_exhausted'}}\n",
      "\n",
      "(retry later in 5 seconds...) -> [ERROR] [error] 429 resource_exhausted. {'error': {'code': 429, 'message': 'resource has been exhausted (e.g. check quota).', 'status': 'resource_exhausted'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 318/318 [36:31<00:00,  6.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by difficulty level:\n",
      "easy: 0.75\n",
      "medium: 0.7758620689655172\n",
      "hard: 0.5660377358490566\n",
      "Accuracy by variants:\n",
      "no: 0.7005988023952096\n",
      "Overall accuracy: 0.7005988023952096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████████████████████▍                                                                                          | 61/318 [11:01<55:09, 12.88s/it]"
     ]
    }
   ],
   "source": [
    "test_gemini_on_VisSim_text_inst('2d_text_instruct_test',  '.', model=model,index=0, debug=False)\n",
    "test_gemini_on_VisSim_text_inst('2d_text_instruct_test',  '.', model=model,index=1, debug=False)\n",
    "test_gemini_on_VisSim_text_inst('2d_text_instruct_test',  '.', model=model,index=2, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9e84565a-715f-4873-bbc3-af701ae4a199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 318/318 [00:05<00:00, 60.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by difficulty level:\n",
      "easy: 0.4298245614035088\n",
      "medium: 0.5277777777777778\n",
      "hard: 0.4166666666666667\n",
      "Accuracy by variants:\n",
      "no: 0.4591194968553459\n",
      "Overall accuracy: 0.4591194968553459\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = 'gemini-2.0-flash-thinking-exp-01-21'\n",
    "dataset_name = '2d_text_instruct_test'\n",
    "import numpy as np\n",
    "answer_dict = np.load(f\"gemini-2.0-flash-thinking-exp-01-21_{dataset_name}_2.json.npy\", allow_pickle=True).item() \n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"VisSim/{dataset_name}\")\n",
    "dataset = dataset['train']\n",
    "d_index = list(range(len(dataset)))\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "acc_by_type = defaultdict(float)\n",
    "acc_by_difficulty = defaultdict(float)\n",
    "counts_by_difficulty = defaultdict(int)\n",
    "counts_by_type = defaultdict(int)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(d_index, total=len(d_index)):\n",
    "    example = dataset[i]\n",
    "    qid = example['qid']\n",
    "    \n",
    "    difficulty_level = example['difficulty_level']\n",
    "    variant = \" \".join(qid.split(\"_\")[1:-1])\n",
    "    \n",
    "    gt_ans = example['answer']\n",
    "\n",
    "    pred = answer_dict[qid]['pred']\n",
    "    if pred == \"Z\":\n",
    "        print(qid, pred, answer_dict[qid]['respone'])\n",
    "        \n",
    "    \n",
    " \n",
    "    if \"ans\" not in variant:\n",
    "        acc_by_difficulty[difficulty_level]+= pred.lower() == gt_ans.lower()\n",
    "        counts_by_difficulty[difficulty_level] += 1\n",
    "    acc_by_type[variant] += pred.lower() == gt_ans.lower()\n",
    "    counts_by_type[variant] += 1\n",
    "\n",
    "# print accuracy\n",
    "np.save(output_path, answer_dict)\n",
    "print(\"Accuracy by difficulty level:\")\n",
    "for k, v in acc_by_difficulty.items():\n",
    "    print(f\"{k}: {v/counts_by_difficulty[k]}\")\n",
    "\n",
    "print(\"Accuracy by variants:\")\n",
    "for k, v in acc_by_type.items():\n",
    "    print(f\"{k}: {v/counts_by_type[k]}\")\n",
    "\n",
    "overall_acc = sum(acc_by_difficulty.values())/sum(counts_by_difficulty.values())\n",
    "print(f\"Overall accuracy: {overall_acc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e2cd23-b609-4343-9455-bc1191b6d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gemini_on_folding_nets('tangram_puzzle_test', '.', model=model, index=0, debug=False)\n",
    "test_gemini_on_folding_nets('tangram_puzzle_test', '.', model=model, index=1, debug=False)\n",
    "test_gemini_on_folding_nets('tangram_puzzle_test', '.', model=model, index=2, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "27fb7bc7-1307-41ce-9e1e-c2ac9f1a4425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 376/376 [00:06<00:00, 54.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 by variants:\n",
      "q+steps: 0.5319283599776713\n",
      "q_only: 0.660537790245632\n",
      "Random Chance F1 by variants:\n",
      "q+steps: 0.5054152815698955\n",
      "q_only: 0.510852133005453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = 'gemini-2.0-flash-thinking-exp-01-21'\n",
    "dataset_name = 'tangram_puzzle_test'\n",
    "index= 0\n",
    "import numpy as np\n",
    "answer_dict = np.load(f\"gemini-2.0-flash-thinking-exp-01-21_{dataset_name}_0.json.npy\", allow_pickle=True).item() \n",
    "print(len(answer_dict))\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"VisSim/{dataset_name}\")\n",
    "dataset = dataset['train']\n",
    "d_index = list(range(len(dataset)))\n",
    "\n",
    "from collections import defaultdict\n",
    "pred_by_type = defaultdict(list)\n",
    "pred_by_difficulty = defaultdict(list)\n",
    "\n",
    "gt_by_type = defaultdict(list)\n",
    "gt_by_difficulty = defaultdict(list)\n",
    "\n",
    "output_path = output_path = f\"./{model}_{dataset_name}_{index}.json\"\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(d_index, total=len(d_index)):\n",
    "    example = dataset[i]\n",
    "    qid = example['qid']\n",
    "    variant = example['type']\n",
    "\n",
    "    \n",
    "    pred = answer_dict[qid]['pred']\n",
    "    if pred == \"Z\":\n",
    "        print(qid, pred, answer_dict[qid]['response'])\n",
    "\n",
    "    gt_choice = example['answer'].lower()\n",
    "    answer_choices = example['choices']\n",
    "\n",
    "    gt_ans = answer_choices[int(ord(gt_choice) - ord('a'))]\n",
    "\n",
    "    ect = pred.lower() == gt_ans.lower() or pred.lower() == gt_choice.lower()\n",
    "    pred_by_type[variant].append(pred.lower())\n",
    "    gt_by_type[variant].append(gt_ans.lower())\n",
    "\n",
    "print(\"F1 by variants:\")\n",
    "for k in pred_by_type.keys():\n",
    "    from sklearn.metrics import f1_score\n",
    "    print(f\"{k}: {f1_score(gt_by_type[k], pred_by_type[k], average='weighted')}\")\n",
    "\n",
    "print(\"Random Chance F1 by variants:\")\n",
    "for k in pred_by_type.keys():\n",
    "    from sklearn.metrics import f1_score\n",
    "    import random\n",
    "    random_pred = [random.choice([\"yes\", \"no\"]) for _ in range(len(gt_by_type[k]))]\n",
    "    print(f\"{k}: {f1_score(gt_by_type[k],random_pred, average='weighted')}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f48857-14d7-4323-9425-bd9f332b4834",
   "metadata": {},
   "source": [
    "# Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87010cb3-b55b-4ec1-bb20-9625c1359754",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import anthropic\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "client = anthropic.Anthropic()\n",
    "import base64\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "def pil_to_base64(pil_image):\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_encoded_str = base64.b64encode(img_byte_arr.getvalue()).decode('ascii')\n",
    "    return img_encoded_str\n",
    "\n",
    "\n",
    "def claude_call_single(query, model):\n",
    "    message = client.messages.create(\n",
    "            model= model,\n",
    "            max_tokens=1024,\n",
    "            \n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": query\n",
    "                   \n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "    return message.content[0].text\n",
    "\n",
    "\n",
    "def extract_answer_from_model_response(model_response):\n",
    "    import re\n",
    "    match = re.search(r'\\\\boxed\\{.*?\\b([A-D]|yes|no)\\b.*?\\}', model_response)\n",
    "    \n",
    "    return match.group(1) if match else \"Z\"\n",
    "\n",
    "\n",
    "# convert PIL image to base64\n",
    "def pil_to_base64(pil_image):\n",
    "    import io\n",
    "    import base64\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_encoded_str = base64.b64encode(img_byte_arr.getvalue()).decode('ascii')\n",
    "    return img_encoded_str\n",
    "\n",
    "\n",
    "\n",
    "def test_claude_on_VisSim_va(dataset_name, output_dir, model, index, max_tokens=2048, debug=False):\n",
    "\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(f\"VisSim/{dataset_name}\")\n",
    "    dataset = dataset['train']\n",
    "    d_index = list(range(len(dataset)))\n",
    "\n",
    "    \n",
    "    output_path = f\"./{model}_{dataset_name}_{index}.json\"\n",
    "    from collections import defaultdict\n",
    "    acc_by_type = defaultdict(float)\n",
    "    acc_by_difficulty = defaultdict(float)\n",
    "    counts_by_difficulty = defaultdict(int)\n",
    "    counts_by_type = defaultdict(int)\n",
    "\n",
    "    # create output dir\n",
    "    answer_dict= {}\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(d_index, total=len(d_index)):\n",
    "        example = dataset[i]\n",
    "        qid = example['qid']\n",
    "        difficulty_level = example['difficulty_level']\n",
    "        variant = \" \".join(qid.split(\"_\")[1:-1])\n",
    "        # 'Observe the transformation pattern of Shape A through steps 0 to 1. <question_image> Apply the same transformation sequence to Shape B and determine the final shape at step 3. <image_for_B> For reference, the black dots in each panel of the figures indicate the origin. Select the correct answer choice that matches the expected transformation result. <answer_choices>'\n",
    "        A_image = example['A_image']\n",
    "        B_image = example['B_image']\n",
    "        question_info = json.loads(example['question_info'])\n",
    "        question = question_info['question']\n",
    "        choice_image = example['choices']\n",
    "        query = []\n",
    "        prefix, question = question.strip().split(\"<question_image>\")\n",
    "        query.append({\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prefix\n",
    "                        })\n",
    "        query.append({\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": \"image/png\",\n",
    "                                \"data\": pil_to_base64(A_image),\n",
    "                            }})\n",
    "        prefix, question = question.split(\"<image_for_B>\")\n",
    "        query.append({\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prefix\n",
    "                        })\n",
    "        query.append({\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": \"image/png\",\n",
    "                                \"data\": pil_to_base64(B_image),\n",
    "                            }})\n",
    "        prefix, question = question.split(\"<answer_choices>\")   \n",
    "        query.append({\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prefix\n",
    "                        })\n",
    "        query.append({\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": \"image/png\",\n",
    "                                \"data\": pil_to_base64(choice_image),\n",
    "                            }})\n",
    "        if len(question) > 0:\n",
    "            query.append({\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": question\n",
    "                        })\n",
    "        query.append({\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"Please first solve the problem step by step, then put your final answer or a single letter (if it is a multiple choice question) in one \\\"\\\\boxed{}\\\"\"\n",
    "                        })\n",
    "        \n",
    "        # print(query)\n",
    "        response = claude_call_single(query, model=model)\n",
    "\n",
    "        pred = extract_answer_from_model_response(response)\n",
    "\n",
    "        gt_ans = example['answer']\n",
    "\n",
    "        answer_dict[qid] ={\n",
    "            \"pred\": pred,\n",
    "            \"gt_ans\": gt_ans.lower(),\n",
    "            \"response\": response\n",
    "        }\n",
    "        if \"ans\" not in variant:\n",
    "            acc_by_difficulty[difficulty_level]+= pred.lower() == gt_ans.lower()\n",
    "            counts_by_difficulty[difficulty_level] += 1\n",
    "        acc_by_type[variant] += pred.lower() == gt_ans.lower()\n",
    "        counts_by_type[variant] += 1\n",
    "\n",
    "        if len(answer_dict)%10==0:\n",
    "            np.save(output_path, answer_dict)\n",
    "\n",
    "    # print accuracy\n",
    "    print(dataset, index)\n",
    "    print(\"Accuracy by difficulty level:\")\n",
    "    for k, v in acc_by_difficulty.items():\n",
    "        print(f\"{k}: {v/counts_by_difficulty[k]}\")\n",
    "\n",
    "    print(\"Accuracy by variants:\")\n",
    "    for k, v in acc_by_type.items():\n",
    "        print(f\"{k}: {v/counts_by_type[k]}\")\n",
    "    \n",
    "    overall_acc = sum(acc_by_difficulty.values())/sum(counts_by_difficulty.values())\n",
    "    print(f\"Overall accuracy: {overall_acc}\")\n",
    "    np.save(output_path, answer_dict)\n",
    "\n",
    "def test_claude_on_VisSim_text_inst(dataset_name, output_dir, model, index, max_tokens=2048,debug=False):\n",
    "\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(f\"VisSim/{dataset_name}\")\n",
    "    dataset = dataset['train']\n",
    "    d_index = list(range(len(dataset)))\n",
    "   \n",
    "\n",
    "\n",
    "    from collections import defaultdict\n",
    "    acc_by_type = defaultdict(float)\n",
    "    acc_by_difficulty = defaultdict(float)\n",
    "    counts_by_difficulty = defaultdict(int)\n",
    "    counts_by_type = defaultdict(int)\n",
    "\n",
    "    output_path  = f\"./{model}_{dataset_name}_{index}.json\"\n",
    "    if os.path.exists(output_path+\".npy\"):\n",
    "        answer_dict = np.load(output_path+\".npy\", allow_pickle=True).item()\n",
    "    else:\n",
    "        answer_dict={}\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(d_index, total=len(d_index)):\n",
    "        example = dataset[i]\n",
    "        qid = example['qid']\n",
    "        if qid in answer_dict:\n",
    "            continue\n",
    "        difficulty_level = example['difficulty_level']\n",
    "        variant = \" \".join(qid.split(\"_\")[1:-1])\n",
    "        \n",
    "        images = example['images'][:-1]\n",
    "        # question_info = json.loads(example['question_info'])\n",
    "        question = example['question']\n",
    "        choice_image = example['choices']\n",
    "\n",
    "        # use regex to parse the question and place the images in the right spots\n",
    "        query = []\n",
    "        for i, image in enumerate(images):\n",
    "            if i == 0:\n",
    "                prefix, question = question.strip().split(\"<shapeB_image>\")\n",
    "            else:\n",
    "                prefix, question = question.split(f\"<shapeB_step_{i-1}>\")\n",
    "            query.append({\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prefix\n",
    "                        })\n",
    "            query.append({\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": \"image/png\",\n",
    "                                \"data\": pil_to_base64(image),\n",
    "                            }})\n",
    "        \n",
    "        # replace the remaining <shapeB_image> with \"\" using regex\n",
    "        import re\n",
    "        # using wildcards to match the <shapeB_step_{i}> and replace it with \"\"\n",
    "        uery.append({\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": re.sub(r'<shapeB_step_\\d+>', '', question)\n",
    "                        })\n",
    "        query.append({\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": \"image/png\",\n",
    "                                \"data\": pil_to_base64(choice_image),\n",
    "                            }})\n",
    "        \n",
    "        query.append({\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"Please first solve the problem step by step, then put your final answer or a single letter (if it is a multiple choice question) in one \\\"\\\\boxed{}\\\"\"\n",
    "                        })\n",
    "        response = claude_call_single(query, model=model)\n",
    "\n",
    "        pred = extract_answer_from_model_response(response)\n",
    "\n",
    "        gt_ans = example['answer']\n",
    "\n",
    "        answer_dict[qid]={\n",
    "            \"pred\": pred,\n",
    "            \"gt_ans\": gt_ans.lower(),\n",
    "            \"response\": response\n",
    "        }\n",
    "        if len(answer_dict)%10==0:\n",
    "            np.save(output_path, answer_dict)\n",
    "        if \"ans\" not in variant:\n",
    "            acc_by_difficulty[difficulty_level]+= pred.lower() == gt_ans.lower()\n",
    "            counts_by_difficulty[difficulty_level] += 1\n",
    "        acc_by_type[variant] += pred.lower() == gt_ans.lower()\n",
    "        counts_by_type[variant] += 1\n",
    "\n",
    "    # print accuracy\n",
    "    np.save(output_path, answer_dict)\n",
    "    print(\"Accuracy by difficulty level:\")\n",
    "    for k, v in acc_by_difficulty.items():\n",
    "        print(f\"{k}: {v/counts_by_difficulty[k]}\")\n",
    "\n",
    "    print(\"Accuracy by variants:\")\n",
    "    for k, v in acc_by_type.items():\n",
    "        print(f\"{k}: {v/counts_by_type[k]}\")\n",
    "    \n",
    "    overall_acc = sum(acc_by_difficulty.values())/sum(counts_by_difficulty.values())\n",
    "    print(f\"Overall accuracy: {overall_acc}\")\n",
    "\n",
    "\n",
    "\n",
    "def test_claude_on_folding_nets(dataset_name, output_dir, model, index, max_tokens=2048,debug=False):\n",
    "\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(f\"VisSim/{dataset_name}\")\n",
    "    dataset = dataset['train']\n",
    "    d_index = list(range(len(dataset)))\n",
    "\n",
    "\n",
    "    from collections import defaultdict\n",
    "    pred_by_type = defaultdict(list)\n",
    "    pred_by_difficulty = defaultdict(list)\n",
    "\n",
    "    gt_by_type = defaultdict(list)\n",
    "    gt_by_difficulty = defaultdict(list)\n",
    "\n",
    "    \n",
    "    output_path  = f\"./{model}_{dataset_name}_{index}.json\"\n",
    "    if os.path.exists(output_path+\".npy\"):\n",
    "        answer_dict = np.load(output_path+\".npy\", allow_pickle=True).item()\n",
    "    else:\n",
    "        answer_dict={}\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(d_index, total=len(d_index)):\n",
    "        example = dataset[i]\n",
    "        qid = example['qid']\n",
    "        if qid in answer_dict:\n",
    "            continue\n",
    "        variant = example['type']\n",
    "  \n",
    "        images = example['images']\n",
    "        # question_info = json.loads(example['question_info'])\n",
    "        question = example['question']\n",
    "\n",
    "        # use regex to parse the question and place the images in the right spots\n",
    "        query = []\n",
    "        for i, image in enumerate(images):\n",
    "            prefix, question = question.split(f\"<image_{i}>\")\n",
    "            query.append({\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prefix\n",
    "                        })\n",
    "            query.append({\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": \"image/png\",\n",
    "                                \"data\": pil_to_base64(image),\n",
    "                            }})\n",
    "        if len(question) > 0:\n",
    "            query.append({\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": question + \"Think step-by-step, and then put your final answer in \\\"\\\\boxed{}\\\".\"\n",
    "                        })\n",
    "        else:\n",
    "            query.append({\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"Think step-by-step, and then put your final answer in \\\"\\\\boxed{}\\\".\"\n",
    "                        })\n",
    "        \n",
    "        \n",
    "        response = claude_call_single(query, model=model)\n",
    "\n",
    "        pred = extract_answer_from_model_response(response.lower())\n",
    "\n",
    "        gt_choice = example['answer'].lower()\n",
    "        answer_choices = example['choices']\n",
    "\n",
    "        gt_ans = answer_choices[int(ord(gt_choice) - ord('a'))]\n",
    "\n",
    "        answer_dict[ qid]={\n",
    "            \"pred\": pred,\n",
    "            \"gt_choice\": gt_choice,\n",
    "            \"gt_ans\": gt_ans.lower(),\n",
    "            \"response\": response,\n",
    "            \"question\": [q  if isinstance(q, str) else '<image>' for q in query]\n",
    "        }\n",
    "        if len(answer_dict)%10==0:\n",
    "            np.save(output_path, answer_dict)\n",
    "        correct = pred.lower() == gt_ans.lower() or pred.lower() == gt_choice.lower()\n",
    "        pred_by_type[variant].append(pred.lower())\n",
    "        gt_by_type[variant].append(gt_ans.lower())\n",
    "\n",
    "    np.save(output_path, answer_dict)\n",
    "    print(\"F1 by variants:\")\n",
    "    for k in pred_by_type.keys():\n",
    "        from sklearn.metrics import f1_score\n",
    "        print(f\"{k}: {f1_score(gt_by_type[k], pred_by_type[k], average='weighted')}\")\n",
    "\n",
    "    print(\"Random Chance F1 by variants:\")\n",
    "    for k in pred_by_type.keys():\n",
    "        from sklearn.metrics import f1_score\n",
    "        import random\n",
    "        random_pred = [random.choice([\"yes\", \"no\"]) for _ in range(len(gt_by_type[k]))]\n",
    "        print(f\"{k}: {f1_score(gt_by_type[k],random_pred, average='weighted')}\")\n",
    "\n",
    "\n",
    "model = \"claude-3-5-sonnet-20241022\"\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "178a96fd-4aad-4069-906a-ee8215b46a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 306/306 [39:00<00:00,  7.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['qid', 'A_image', 'B_image', 'choices', 'answer', 'transformations', 'difficulty_level', 'question_info', 'answer_info'],\n",
      "    num_rows: 306\n",
      "}) 0\n",
      "Accuracy by difficulty level:\n",
      "easy: 0.7549019607843137\n",
      "medium: 0.6666666666666666\n",
      "hard: 0.6274509803921569\n",
      "Accuracy by variants:\n",
      "easy no: 0.7549019607843137\n",
      "medium no: 0.6666666666666666\n",
      "hard no: 0.6274509803921569\n",
      "Overall accuracy: 0.6830065359477124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 306/306 [40:11<00:00,  7.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['qid', 'A_image', 'B_image', 'choices', 'answer', 'transformations', 'difficulty_level', 'question_info', 'answer_info'],\n",
      "    num_rows: 306\n",
      "}) 1\n",
      "Accuracy by difficulty level:\n",
      "easy: 0.803921568627451\n",
      "medium: 0.6862745098039216\n",
      "hard: 0.6176470588235294\n",
      "Accuracy by variants:\n",
      "easy no: 0.803921568627451\n",
      "medium no: 0.6862745098039216\n",
      "hard no: 0.6176470588235294\n",
      "Overall accuracy: 0.7026143790849673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 306/306 [38:17<00:00,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['qid', 'A_image', 'B_image', 'choices', 'answer', 'transformations', 'difficulty_level', 'question_info', 'answer_info'],\n",
      "    num_rows: 306\n",
      "}) 2\n",
      "Accuracy by difficulty level:\n",
      "easy: 0.7352941176470589\n",
      "medium: 0.6470588235294118\n",
      "hard: 0.6176470588235294\n",
      "Accuracy by variants:\n",
      "easy no: 0.7352941176470589\n",
      "medium no: 0.6470588235294118\n",
      "hard no: 0.6176470588235294\n",
      "Overall accuracy: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_claude_on_VisSim_va('2d_va_test', '.', model=model, index=0, debug=False)\n",
    "test_claude_on_VisSim_va('2d_va_test', '.', model=model, index=1, debug=False)\n",
    "test_claude_on_VisSim_va('2d_va_test', '.', model=model, index=2, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d581d5-1230-46d5-ac16-eb26f2bc363f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 376/376 [03:46<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 by variants:\n",
      "q_only: 0.7605042016806722\n",
      "q+steps: 0.4155844155844156\n",
      "Random Chance F1 by variants:\n",
      "q_only: 0.6666666666666666\n",
      "q+steps: 0.253968253968254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 376/376 [48:40<00:00,  7.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 by variants:\n",
      "q+steps: 0.41326203462125793\n",
      "q_only: 0.7231833910034601\n",
      "Random Chance F1 by variants:\n",
      "q+steps: 0.5091154606588453\n",
      "q_only: 0.4995475113122172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████████████████████████████████████████████  | 362/376 [47:18<01:55,  8.22s/it]"
     ]
    }
   ],
   "source": [
    "test_claude_on_folding_nets('tangram_puzzle_test', '.', model=model, index=0, debug=False)\n",
    "test_claude_on_folding_nets('tangram_puzzle_test', '.', model=model, index=1, debug=False)\n",
    "test_claude_on_folding_nets('tangram_puzzle_test', '.', model=model, index=2, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a615ab8c-4090-452e-93bb-1e56ec9786fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_claude_on_VisSim_text_inst('2d_text_instruct_test',  '.', model=model,index=0, debug=False)\n",
    "test_claude_on_VisSim_text_inst('2d_text_instruct_test',  '.', model=model,index=1, debug=False)\n",
    "test_claude_on_VisSim_text_inst('2d_text_instruct_test',  '.', model=model,index=2, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243a60b-97eb-46ed-9d99-67fde60999b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_claude_on_folding_nets('folding_nets_test', '.', model=model, index=0, debug=False)\n",
    "test_claude_on_folding_nets('folding_nets_test', '.', model=model, index=1, debug=False)\n",
    "test_claude_on_folding_nets('folding_nets_test', '.', model=model, index=2, debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
