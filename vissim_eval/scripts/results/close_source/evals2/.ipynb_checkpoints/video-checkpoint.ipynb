{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a59f9d-b1d5-4d19-bc5a-7b7313dc7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f1(gts, preds):\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    f1_macro = f1_score(gts, preds, average='macro')\n",
    "    acc = np.sum(np.array(gts) == np.array(preds)) / len(gts)\n",
    "    return \"acc\", acc, \"f1 macro\", f1_macro\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_letter(text):\n",
    "    # Regular expression to capture a single letter (A, B, C, D) inside \\boxed{}\n",
    "    match = re.search(r'\\\\boxed\\{.*?\\b([A-D])\\b.*?\\}', text)\n",
    "    \n",
    "    return match.group(1) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa0c479f-263a-4b00-b6a1-fe51a0d0951b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46399070b3014912a0b09915756c5290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(retry later in 5 seconds...) ->\n",
      "\n",
      "(retry later in 5 seconds...) ->\n",
      "\n",
      "(retry later...) -> [ERROR] [error] 500 internal. {'error': {'code': 500, 'message': 'an internal error has occurred. please retry or report in https://developers.generativeai.google/guide/troubleshooting', 'status': 'internal'}}\n",
      "\n",
      "(retry later...) -> [ERROR] [error] 500 internal. {'error': {'code': 500, 'message': 'an internal error has occurred. please retry or report in https://developers.generativeai.google/guide/troubleshooting', 'status': 'internal'}}\n",
      "\n",
      "(retry later...) -> [ERROR] [error] 500 internal. {'error': {'code': 500, 'message': 'an internal error has occurred. please retry or report in https://developers.generativeai.google/guide/troubleshooting', 'status': 'internal'}}\n",
      "213 471 45.22292993630573\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from google import genai\n",
    "import matplotlib.pyplot as plt\n",
    "from google.genai import types\n",
    "\n",
    "# import google.generativeai as genai\n",
    "GEMINI_API_KEY = \"AIzaSyCxuedQMQWQkYSu67h3L5PMXDg3cDmeeBQ\"\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def gemini_call_single(query, model='gemini-2.0-flash-thinking-exp-01-21', max_tokens=2048):\n",
    "    # import cv2\n",
    "    import requests\n",
    "    import time\n",
    "    import json\n",
    "\n",
    "    # print(query)\n",
    "\n",
    "    while True:\n",
    "        # print('\\n(trying...) ->', )\n",
    "        try:\n",
    "            \n",
    "            content = client.models.generate_content(\n",
    "                model=model,\n",
    "                contents=query,\n",
    "                # config = types.GenerateContentConfig(\n",
    "                #     temperature=0\n",
    "                # )\n",
    "            )\n",
    "            \n",
    "\n",
    "        except Exception as e_msg:\n",
    "            content = '[ERROR] ' + str(e_msg)\n",
    " \n",
    "        if isinstance(content, str):\n",
    "            content = '[ERROR] ' + content.lower()\n",
    "            if 'exceeded call rate limit' in content or 'exhausted' in content:\n",
    "                # retry for unacceptable response\n",
    "                print('\\n(retry later in 5 seconds...) ->')\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            else:\n",
    "                print('\\n(retry later...) ->', content)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        \n",
    "    ########################################\n",
    "    \n",
    "    # print(responseJson[\"choices\"][0][\"message\"][\"content\"])\n",
    "    return content.text\n",
    "\n",
    "\n",
    "def extract_answer_from_model_response(model_response):\n",
    "    model_answer = model_response.strip()\n",
    "    model_answer = model_answer.replace(\"option\", \"\")\n",
    "    # model_answer = model_answer.replace(\"textbf\", \"text\")\n",
    "    \n",
    "    model_answer = model_answer.split('oxed{')[-1][0]\n",
    "        \n",
    "    # model_answer = find_math_answer(model_answer).replace('(a)', 'a').replace('(b)', 'b').replace('(c)', 'c').replace('(d)', 'd').replace('(e)', 'e').replace('{a}', 'a').replace('{b}', 'b').replace('{c}', 'c').replace('{d}', 'd').replace('{e}', 'e').replace('*', '').rstrip('.').lstrip(':').strip()\n",
    "    return model_answer\n",
    "\n",
    "\n",
    "# convert PIL image to base64\n",
    "def pil_to_base64(pil_image):\n",
    "    import io\n",
    "    import base64\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_encoded_str = base64.b64encode(img_byte_arr.getvalue()).decode('ascii')\n",
    "    return img_encoded_str\n",
    "\n",
    "\n",
    "def test_gemini( ):\n",
    "    # load hf dataset\n",
    "    answer_dict = {}\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(f\"MahtabBg/Video\")\n",
    "    dataset = dataset['train']\n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "    acc, total = 0,0\n",
    "    from tqdm.notebook import tqdm\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        example = dataset[i]\n",
    "        qid = example['id']\n",
    "       \n",
    "        question_image = example['question']\n",
    "        choice_image = example['choices']\n",
    "        \n",
    "        query = [\n",
    "            \"<Image 1>:\",\n",
    "            question_image,\n",
    "            \"<Image 2>:\",\n",
    "            choice_image,\n",
    "             \"You see 4 sequential frames of a video in <Image 1>, but one is missing (marked with '?').\\n Choose which of the images in the <Image 2> correctly fills the missing frame.\\nRemember, the camera only moves in one direction (left or right) in the video.\",\n",
    "            \"\\nPlease first solve the problem step by step, then put your final answer or a single letter (if it is a multiple choice question) in one \\\"\\\\boxed{}\\\"\"\n",
    "        ]\n",
    "\n",
    "        response = gemini_call_single(query)\n",
    "\n",
    "        pred = extract_answer_from_model_response(response)\n",
    "\n",
    "        gt_ans = example['answer']\n",
    "        answer_dict[qid] = {\"response\": response, \"pred\": pred}\n",
    "        # print(qid, gt_ans, answer_dict[qid])\n",
    "        if gt_ans == pred:\n",
    "            acc+=1\n",
    "        total+=1\n",
    "        \n",
    "    print(acc, total, acc/total*100)\n",
    "    if len(answer_dict) %10 ==0:\n",
    "        np.save(\"video_gemini_flash_2_thinking.npy\", answer_dict)\n",
    "    return answer_dict\n",
    "    \n",
    "\n",
    "answer_dict = test_gemini()\n",
    "np.save(\"video_gemini_flash_2_thinking.npy\", answer_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ffea2ed-9b8a-4e06-bea2-5698e149b467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('acc', 0.45222929936305734, 'f1 macro', 0.45039551672546424)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_dict= np.load(\"video_gemini_flash_2_thinking.npy\" ,allow_pickle=True).item()\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"MahtabBg/Video\")\n",
    "dataset = dataset['train']\n",
    "gts, preds =[], []\n",
    "for i in range(len(dataset)):\n",
    "    el = dataset[i]\n",
    "    gts.append(el['answer'].lower())\n",
    "    pred = extract_letter(answer_dict[el['id']]['response']).lower()\n",
    "    assert pred in ['a', 'b', 'c']\n",
    "    if pred!= answer_dict[el['id']]['pred'].lower():\n",
    "        print(pred, answer_dict[el['id']]['pred'].lower(), answer_dict[el['id']]['response'])\n",
    "    preds.append(pred)\n",
    "\n",
    "calc_f1(gts, preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f718851d-e126-432f-986e-808e8ff03ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fbb2257a014c7884d2a0468a96527f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import anthropic\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "client = anthropic.Anthropic()\n",
    "import base64\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "def encode_image(pil_image):\n",
    "    pil_image.save(\"tempclaude.png\")\n",
    "    image_path = \"tempclaude.png\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    return image_data\n",
    "\n",
    "def extract_answer_from_model_response(model_response):\n",
    "    model_answer = model_response.strip()\n",
    "    model_answer = model_answer.replace(\"option\", \"\")\n",
    "    # model_answer = model_answer.replace(\"textbf\", \"text\")\n",
    "    \n",
    "    model_answer = model_answer.split('oxed{')[-1][0]\n",
    "        \n",
    "    # model_answer = find_math_answer(model_answer).replace('(a)', 'a').replace('(b)', 'b').replace('(c)', 'c').replace('(d)', 'd').replace('(e)', 'e').replace('{a}', 'a').replace('{b}', 'b').replace('{c}', 'c').replace('{d}', 'd').replace('{e}', 'e').replace('*', '').rstrip('.').lstrip(':').strip()\n",
    "    return model_answer\n",
    "\n",
    "answer_dict= {}\n",
    "image_media_type = \"image/png\"\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"MahtabBg/Video\")\n",
    "dataset = dataset['train']\n",
    "if True:\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        example = dataset[i]\n",
    "        qid = example['id']\n",
    "       \n",
    "        question_image =  example['question']\n",
    "        choice_image = example['choices']\n",
    "        \n",
    "      \n",
    "        \n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=1024,\n",
    "            # temperature =0,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 1>:\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": image_media_type,\n",
    "                                \"data\": encode_image(question_image),\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 2>:\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": image_media_type,\n",
    "                                \"data\": encode_image(choice_image),\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"You see 4 sequential frames of a video in <Image 1>, but one is missing (marked with '?').\\n Choose which of the images in the <Image 2> correctly fills the missing frame.\\nRemember, the camera only moves in one direction (left or right) in the video. Please first solve the problem step by step, then put your final answer or a single letter (if it is a multiple choice question) in one \\\"\\\\boxed{}\\\"\"\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        # plt.imshow(question_image)\n",
    "        # plt.show()\n",
    "        # plt.imshow(choice_image)\n",
    "        # plt.show()\n",
    "        # print(message.content[0].text)\n",
    "        # break\n",
    "        answer_dict[qid] = { \"response\":message.content[0].text, \"pred\": extract_answer_from_model_response(message.content[0].text)}\n",
    "        if len(answer_dict)%10==0:\n",
    "            np.save(\"video_claude_sonnet.npy\", answer_dict)\n",
    "            \n",
    "        \n",
    "np.save(\"video_claude_sonnet.npy\", answer_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dff54b20-b909-443b-9a9a-167719da10a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me solve this step by step:\n",
      "\n",
      "1) In Image 1, I see a sequence of a white teacup with pink floral design\n",
      "- Frame 1: Cup is positioned normally\n",
      "- Frame 2: Missing (marked with '?')\n",
      "- Frame 3: Cup is slightly tilted\n",
      "- Frame 4: Cup is more tilted to its side\n",
      "\n",
      "2) The sequence shows the cup gradually tilting over from left to right.\n",
      "\n",
      "3) Looking at Image 2's options:\n",
      "- A: Shows cup standing normally\n",
      "- B: Shows cup tilted significantly\n",
      "- C: Shows cup standing normally\n",
      "\n",
      "4) The missing frame should show a slight tilt, more than Frame 1 but less than Frame 3, to maintain a smooth progression of the tipping motion.\n",
      "\n",
      "5) Option A shows the cup too upright to fit the sequence.\n",
      "Option B shows the cup too tilted to fit the sequence.\n",
      "Option C shows the cup too upright to fit the sequence.\n",
      "\n",
      "6) Therefore, none of these options seem to correctly fill the gap in the sequence.\n",
      "\n",
      "\\boxed{None of the above}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me solve this step by step:\n",
      "\n",
      "1) In <Image 1>, I see three frames showing a dark armchair from different angles, followed by a missing frame marked with \"?\"\n",
      "\n",
      "2) Looking at the sequence, the camera appears to be moving from left to right:\n",
      "   - First frame shows the chair from more left side\n",
      "   - Second frame is more centered\n",
      "   - Third frame shows the chair from more right side\n",
      "\n",
      "3) Therefore, the missing fourth frame should continue this rightward movement, showing the chair from an even more rightward angle.\n",
      "\n",
      "4) Looking at options A, B, C in <Image 2>:\n",
      "   - Option A shows the chair from the leftmost angle\n",
      "   - Option B shows the chair from a center angle\n",
      "   - Option C shows the chair from the rightmost angle\n",
      "\n",
      "5) Since we need a frame that continues the rightward movement from frame 3, and C shows the most rightward view, that would disrupt the sequence.\n",
      "\n",
      "6) Looking back at the original sequence, frames 1-3 appear to be identical to A, B, and C respectively.\n",
      "\n",
      "7) Therefore, none of these options would correctly complete the sequence, as we need a frame showing an even more rightward view than C.\n",
      "\n",
      "\\boxed{None of the above}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me solve this step by step:\n",
      "\n",
      "1) In <Image 1>, we see a sequence of frames showing a box of Fibre cereal on a tiled floor.\n",
      "\n",
      "2) The camera appears to be moving around the box, specifically from front to right side.\n",
      "\n",
      "3) The sequence shows:\n",
      "- Frame 1: Missing (?)\n",
      "- Frame 2: Front view of the box\n",
      "- Frame 3: Slightly angled view from right\n",
      "- Frame 4: Side/nutrition label view\n",
      "\n",
      "4) Looking at the options in <Image 2>:\n",
      "- A: Shows the nutrition label side view\n",
      "- B: Shows the front view\n",
      "- C: Shows a slightly angled view from right\n",
      "\n",
      "5) Since the camera movement is continuous in one direction (moving right), and we're looking for the first frame, we need an image that would come before the front view.\n",
      "\n",
      "6) Given the camera movement pattern, the missing first frame should show a slightly left-angled view of the box, which would naturally come before the straight-on front view.\n",
      "\n",
      "7) Looking at the options, none perfectly matches what should be the first frame, but option A shows the nutrition label which would break the continuous movement pattern, and options B and C are already present in the sequence.\n",
      "\n",
      "Therefore, none of the given options would correctly fill the missing frame, as they're all from later in the sequence.\n",
      "\n",
      "\\boxed{None of the above}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('acc', 0.5456475583864119, 'f1 macro', 0.4094522927176681)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_dict= np.load(\"video_claude_sonnet.npy\" ,allow_pickle=True).item()\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"MahtabBg/Video\")\n",
    "dataset = dataset['train']\n",
    "gts, preds =[], []\n",
    "for i in range(len(dataset)):\n",
    "    el = dataset[i]\n",
    "    gts.append(el['answer'].lower())\n",
    "    try:\n",
    "        pred = extract_letter(answer_dict[el['id']]['response']).lower()\n",
    "    except:\n",
    "        print(answer_dict[el['id']]['response'])\n",
    "        pred= input()\n",
    "    assert pred in ['a', 'b', 'c', 'n']\n",
    "    if pred!= answer_dict[el['id']]['pred'].lower():\n",
    "        print(pred, answer_dict[el['id']]['pred'].lower(), answer_dict[el['id']]['response'])\n",
    "    preds.append(pred)\n",
    "\n",
    "calc_f1(gts, preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0a8f24d-13ec-4165-befe-160cb2574b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b2288fe89e4a889dc44c8dc9d7db50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "client = OpenAI()\n",
    "def encode_image(pil_image):\n",
    "    pil_image.save(\"gpt.png\")\n",
    "    image_path = \"gpt.png\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "def extract_answer_from_model_response(model_response):\n",
    "    model_answer = model_response.strip()\n",
    "    model_answer = model_answer.replace(\"option\", \"\")\n",
    "    # model_answer = model_answer.replace(\"textbf\", \"text\")\n",
    "    try:\n",
    "        model_answer = model_answer.split('oxed{')[-1][0]\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return model_answer\n",
    "\n",
    "\n",
    "dataset = load_dataset(f\"MahtabBg/Video\")\n",
    "dataset = dataset['train']\n",
    "\n",
    "\n",
    "answer_dict = {}\n",
    "from datasets import load_dataset\n",
    "\n",
    "if True:\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        example = dataset[i]\n",
    "        qid = example['id']\n",
    "\n",
    "        \n",
    "       \n",
    "        question_image = example['question']\n",
    "        choice_image = example['choices']\n",
    "        \n",
    "        question_image = encode_image(question_image)\n",
    "        choice_image = encode_image(choice_image)\n",
    "        \n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                         {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 1>:\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\":f\"data:image/jpeg;base64,{question_image}\", \"detail\": \"auto\",\n",
    "                            },\n",
    "                        },\n",
    "                         {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 2>:\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{choice_image}\", \"detail\": \"auto\",\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"You see 4 sequential frames of a video in <Image 1>, but one is missing (marked with '?').\\n Choose which of the images in <Image 2> correctly fills the missing frame.\\nRemember, the camera only moves in one direction (left or right) in the video. Please first solve the problem step by step, then put your final answer or a single letter (if it is a multiple choice question) in one \\\"\\\\boxed{}\\\".\",\n",
    "                        },\n",
    "                        \n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "            # temperature=0,\n",
    "        )\n",
    "        # plt.imshow(example['question'])\n",
    "        # plt.show()\n",
    "        # plt.imshow(example['choices'])\n",
    "        # plt.show()\n",
    "        # print(response.choices[0].message.content)\n",
    "        # break\n",
    "        \n",
    "        answer_dict[qid] = {\"response\":response.choices[0].message.content , \"pred\": extract_answer_from_model_response(response.choices[0].message.content)}\n",
    "        if len(answer_dict)%10==0:\n",
    "            np.save(\"video_gpt4o.npy\", answer_dict)\n",
    "np.save(\"video_gpt4o.npy\", answer_dict)     \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85522d9a-703f-41d7-b575-7480defa23b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('acc', 0.39490445859872614, 'f1 macro', 0.3847784161811909)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_dict= np.load(\"video_gpt4o.npy\" ,allow_pickle=True).item()\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"MahtabBg/Video\")\n",
    "dataset = dataset['train']\n",
    "gts, preds =[], []\n",
    "for i in range(len(dataset)):\n",
    "    el = dataset[i]\n",
    "    gts.append(el['answer'].lower())\n",
    "    pred = extract_letter(answer_dict[el['id']]['response']).lower()\n",
    "    assert pred in ['a', 'b', 'c']\n",
    "    # if pred!= answer_dict[el['id']]['pred'].lower():\n",
    "    #     print(pred, answer_dict[el['id']]['pred'].lower(), answer_dict[el['id']]['response'])\n",
    "    preds.append(pred)\n",
    "\n",
    "calc_f1(gts, preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a947a-0683-4dc8-abe9-f2c31efa61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "client = OpenAI()\n",
    "def encode_image(pil_image, name):\n",
    "    pil_image.save(name)\n",
    "    image_path = name\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "def extract_answer_from_model_response(model_response):\n",
    "    model_answer = model_response.strip()\n",
    "    model_answer = model_answer.replace(\"option\", \"\")\n",
    "    # model_answer = model_answer.replace(\"textbf\", \"text\")\n",
    "    try:\n",
    "        model_answer = model_answer.split('oxed{')[-1][0]\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return model_answer\n",
    "\n",
    "\n",
    "dataset = load_dataset(f\"MahtabBg/Video\")\n",
    "dataset = dataset['train']\n",
    "\n",
    "\n",
    "answer_dict = {}\n",
    "from datasets import load_dataset\n",
    "\n",
    "if True:\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        example = dataset[i]\n",
    "        qid = example['id']\n",
    "\n",
    "        \n",
    "       \n",
    "        question_image = example['question']\n",
    "        choice_image = example['choices']\n",
    "        option_a = example['choice0']\n",
    "        option_b = example['choice1']\n",
    "        option_c = example['choice2']\n",
    "        \n",
    "        question_image = encode_image(question_image, \"qgptvideo.png\")\n",
    "        # choice_image = encode_image(choice_image)\n",
    "        option_a = encode_image(option_a, \"optionagptvideo.png\")\n",
    "        option_b = encode_image(option_b, \"optionbgptvideo.png\")\n",
    "        option_c = encode_image(option_c, \"optioncgptvideo.png\")\n",
    "        \n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                         {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 1>:\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\":f\"data:image/jpeg;base64,{question_image}\", \"detail\": \"auto\",\n",
    "                            },\n",
    "                        },\n",
    "                         {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Option A>:\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{option_a}\", \"detail\": \"auto\",\n",
    "                            },\n",
    "                        },\n",
    "                         {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Option B>:\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{option_b}\", \"detail\": \"auto\",\n",
    "                            },\n",
    "                        },\n",
    "                         {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Option C>:\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{option_c}\", \"detail\": \"auto\",\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"You see 4 sequential frames of a video in <Image 1>, but one is missing (marked with '?').\\n Choose which of the options, <Option A>, <Option B>, <Option C>, correctly fills the missing frame.\\nRemember, the camera only moves in one direction (left or right) in the video. Please first solve the problem step by step, then put your final answer as a single letter in one \\\"\\\\boxed{}\\\".\",\n",
    "                        },\n",
    "                        \n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "            # temperature=0,\n",
    "        )\n",
    "        # plt.imshow(example['question'])\n",
    "        # plt.show()\n",
    "        # plt.imshow(example['choices'])\n",
    "        # plt.show()\n",
    "        # plt.imshow(example['choice0'])\n",
    "        # plt.show()\n",
    "        # plt.imshow(example['choice1'])\n",
    "        # plt.show()\n",
    "        # plt.imshow(example['choice2'])\n",
    "        # plt.show()\n",
    "        # print(response.choices[0].message.content)\n",
    "        # break\n",
    "        \n",
    "        answer_dict[qid] = {\"response\":response.choices[0].message.content , \"pred\": extract_answer_from_model_response(response.choices[0].message.content)}\n",
    "        if len(answer_dict)%10==0:\n",
    "            np.save(\"video_gpt4o_multiimage.npy\", answer_dict)\n",
    "np.save(\"video_gpt4o_multiimage.npy\", answer_dict)     \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f623e5f-d68a-4c83-9249-f457cfb43d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dict= np.load(\"video_gpt4o_multiimage.npy\" ,allow_pickle=True).item()\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"MahtabBg/Video\")\n",
    "dataset = dataset['train']\n",
    "gts, preds =[], []\n",
    "for i in range(len(dataset)):\n",
    "    el = dataset[i]\n",
    "    gts.append(el['answer'].lower())\n",
    "    pred = extract_letter(answer_dict[el['id']]['response']).lower()\n",
    "    assert pred in ['a', 'b', 'c']\n",
    "    # if pred!= answer_dict[el['id']]['pred'].lower():\n",
    "    #     print(pred, answer_dict[el['id']]['pred'].lower(), answer_dict[el['id']]['response'])\n",
    "    preds.append(pred)\n",
    "\n",
    "calc_f1(gts, preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b23b1-d1a7-471f-a656-149178c65fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83528d7fff7441ab960bfad0994be0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import anthropic\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "client = anthropic.Anthropic()\n",
    "import base64\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "def encode_image(pil_image, name):\n",
    "    pil_image.save(name)\n",
    "    image_path = name\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    return image_data\n",
    "\n",
    "def extract_answer_from_model_response(model_response):\n",
    "    model_answer = model_response.strip()\n",
    "    model_answer = model_answer.replace(\"option\", \"\")\n",
    "    # model_answer = model_answer.replace(\"textbf\", \"text\")\n",
    "    \n",
    "    model_answer = model_answer.split('oxed{')[-1][0]\n",
    "        \n",
    "    # model_answer = find_math_answer(model_answer).replace('(a)', 'a').replace('(b)', 'b').replace('(c)', 'c').replace('(d)', 'd').replace('(e)', 'e').replace('{a}', 'a').replace('{b}', 'b').replace('{c}', 'c').replace('{d}', 'd').replace('{e}', 'e').replace('*', '').rstrip('.').lstrip(':').strip()\n",
    "    return model_answer\n",
    "\n",
    "answer_dict= {}\n",
    "image_media_type = \"image/png\"\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"MahtabBg/Video\")\n",
    "dataset = dataset['train']\n",
    "if True:\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        example = dataset[i]\n",
    "        qid = example['id']\n",
    "       \n",
    "        question_image =  example['question']\n",
    "        option_a = example['choice0']\n",
    "        option_b = example['choice1']\n",
    "        option_c = example['choice2']\n",
    "        \n",
    "      \n",
    "        \n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=1024,\n",
    "            # temperature =0,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 1>:\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": image_media_type,\n",
    "                                \"data\": encode_image(question_image, \"qcv.png\"),\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Option A>:\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": image_media_type,\n",
    "                                \"data\": encode_image(option_a, \"optionacv.png\"),\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Option B>:\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": image_media_type,\n",
    "                                \"data\": encode_image(option_b, \"optionbcv.png\"),\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Option C>:\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": image_media_type,\n",
    "                                \"data\": encode_image(option_c, \"optionccv.png\"),\n",
    "                            },\n",
    "                        },\n",
    "                \n",
    "                        \n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"You see 4 sequential frames of a video in <Image 1>, but one is missing (marked with '?').\\n Choose which of the options, <Option A>, <Option B>, <Option C>, correctly fills the missing frame.\\nRemember, the camera only moves in one direction (left or right) in the video. Please first solve the problem step by step, then put your final answer as a single letter in one \\\"\\\\boxed{}\\\"\"\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        # plt.imshow(question_image)\n",
    "        # plt.show()\n",
    "        # plt.imshow(choice_image)\n",
    "        # plt.show()\n",
    "        # print(message.content[0].text)\n",
    "        # break\n",
    "        answer_dict[qid] = { \"response\":message.content[0].text, \"pred\": extract_answer_from_model_response(message.content[0].text)}\n",
    "        if len(answer_dict)%10==0:\n",
    "            np.save(\"video_claude_sonnet_multiimage.npy\", answer_dict)\n",
    "            \n",
    "        \n",
    "np.save(\"video_claude_sonnet_multiimage.npy\", answer_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d0e791-ba34-44f3-a07c-0845e89c0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from google import genai\n",
    "import matplotlib.pyplot as plt\n",
    "from google.genai import types\n",
    "\n",
    "# import google.generativeai as genai\n",
    "GEMINI_API_KEY = \"AIzaSyCxuedQMQWQkYSu67h3L5PMXDg3cDmeeBQ\"\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def gemini_call_single(query, model='gemini-2.0-flash-thinking-exp-01-21', max_tokens=2048):\n",
    "    # import cv2\n",
    "    import requests\n",
    "    import time\n",
    "    import json\n",
    "\n",
    "    # print(query)\n",
    "\n",
    "    while True:\n",
    "        # print('\\n(trying...) ->', )\n",
    "        try:\n",
    "            \n",
    "            content = client.models.generate_content(\n",
    "                model=model,\n",
    "                contents=query,\n",
    "                # config = types.GenerateContentConfig(\n",
    "                #     temperature=0\n",
    "                # )\n",
    "            )\n",
    "            \n",
    "\n",
    "        except Exception as e_msg:\n",
    "            content = '[ERROR] ' + str(e_msg)\n",
    " \n",
    "        if isinstance(content, str):\n",
    "            content = '[ERROR] ' + content.lower()\n",
    "            if 'exceeded call rate limit' in content or 'exhausted' in content:\n",
    "                # retry for unacceptable response\n",
    "                print('\\n(retry later in 5 seconds...) ->')\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            else:\n",
    "                print('\\n(retry later...) ->', content)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        \n",
    "    ########################################\n",
    "    \n",
    "    # print(responseJson[\"choices\"][0][\"message\"][\"content\"])\n",
    "    return content.text\n",
    "\n",
    "\n",
    "def extract_answer_from_model_response(model_response):\n",
    "    model_answer = model_response.strip()\n",
    "    model_answer = model_answer.replace(\"option\", \"\")\n",
    "    # model_answer = model_answer.replace(\"textbf\", \"text\")\n",
    "    \n",
    "    model_answer = model_answer.split('oxed{')[-1][0]\n",
    "        \n",
    "    # model_answer = find_math_answer(model_answer).replace('(a)', 'a').replace('(b)', 'b').replace('(c)', 'c').replace('(d)', 'd').replace('(e)', 'e').replace('{a}', 'a').replace('{b}', 'b').replace('{c}', 'c').replace('{d}', 'd').replace('{e}', 'e').replace('*', '').rstrip('.').lstrip(':').strip()\n",
    "    return model_answer\n",
    "\n",
    "\n",
    "# convert PIL image to base64\n",
    "def pil_to_base64(pil_image):\n",
    "    import io\n",
    "    import base64\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_encoded_str = base64.b64encode(img_byte_arr.getvalue()).decode('ascii')\n",
    "    return img_encoded_str\n",
    "\n",
    "\n",
    "def test_gemini( ):\n",
    "    # load hf dataset\n",
    "    answer_dict = {}\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(f\"MahtabBg/Video\")\n",
    "    dataset = dataset['train']\n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "    acc, total = 0,0\n",
    "    from tqdm.notebook import tqdm\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        example = dataset[i]\n",
    "        qid = example['id']\n",
    "       \n",
    "        question_image = example['question']\n",
    "        option_a = example['choice0']\n",
    "        option_b = example['choice1']\n",
    "        option_c = example['choice2']\n",
    "        \n",
    "        query = [\n",
    "            \"<Image 1>:\",\n",
    "            question_image,\n",
    "            \"<Option A>:\",\n",
    "            option_a,\n",
    "            \"<Option B>:\",\n",
    "            option_b,\n",
    "            \"<Option C>:\",\n",
    "            option_c,\n",
    "             \"You see 4 sequential frames of a video in <Image 1>, but one is missing (marked with '?').\\n Choose which of the options, <Option A>, <Option B>, <Option C>, correctly fills the missing frame.\\nRemember, the camera only moves in one direction (left or right) in the video.\",\n",
    "            \"\\nPlease first solve the problem step by step, then put your final answer as a single letter in one \\\"\\\\boxed{}\\\"\"\n",
    "        ]\n",
    "\n",
    "        response = gemini_call_single(query)\n",
    "\n",
    "        pred = extract_answer_from_model_response(response)\n",
    "\n",
    "        gt_ans = example['answer']\n",
    "        answer_dict[qid] = {\"response\": response, \"pred\": pred}\n",
    "        # print(qid, gt_ans, answer_dict[qid])\n",
    "        if gt_ans == pred:\n",
    "            acc+=1\n",
    "        total+=1\n",
    "        \n",
    "    print(acc, total, acc/total*100)\n",
    "    if len(answer_dict) %10 ==0:\n",
    "        np.save(\"video_gemini_flash_2_thinking_multiimage.npy\", answer_dict)\n",
    "    return answer_dict\n",
    "    \n",
    "\n",
    "answer_dict = test_gemini()\n",
    "np.save(\"video_gemini_flash_2_thinking_multiimage.npy\", answer_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
