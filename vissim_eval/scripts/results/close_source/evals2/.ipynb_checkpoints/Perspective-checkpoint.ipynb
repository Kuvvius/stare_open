{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b3974-30d2-4744-b8d7-e24a3af3dd85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4223199fa2f4d5695b08ed10107e179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "client = OpenAI()\n",
    "def encode_image(pil_image):\n",
    "    pil_image.save(\"gptpers.png\")\n",
    "    image_path = \"gptpers.png\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "def extract_answer_from_model_response(model_response):\n",
    "    model_answer = model_response.strip()\n",
    "    model_answer = model_answer.replace(\"option\", \"\")\n",
    "    # model_answer = model_answer.replace(\"textbf\", \"text\")\n",
    "    try:\n",
    "        model_answer = model_answer.split('oxed{')[-1][0]\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    # model_answer = find_math_answer(model_answer).replace('(a)', 'a').replace('(b)', 'b').replace('(c)', 'c').replace('(d)', 'd').replace('(e)', 'e').replace('{a}', 'a').replace('{b}', 'b').replace('{c}', 'c').replace('{d}', 'd').replace('{e}', 'e').replace('*', '').rstrip('.').lstrip(':').strip()\n",
    "    return model_answer\n",
    "\n",
    "\n",
    "answer_dict = {}\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"MahtabBg/NEWPerspective\")\n",
    "dataset = dataset['train']\n",
    "if True:\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        example = dataset[i]\n",
    "        qid = example['id']\n",
    "\n",
    "        question_image = example['topdown']\n",
    "        choice_image = example['choices']\n",
    "        question_image = encode_image(question_image)\n",
    "        choice_image = encode_image(choice_image)\n",
    "        \n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 1>:\",\n",
    "                        },\n",
    "\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\":f\"data:image/jpeg;base64,{question_image}\", \"detail\": \"auto\",\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 2>:\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{choice_image}\", \"detail\": \"auto\",\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 1> shows an image from the top of a scene with a red square indicating an agent and a red arrow indicating the agent's direction of view.\\nSelect from the <Image 2> which image represents the agent's view. Please first solve the problem step by step, then put your final answer or a single letter (if it is a multiple choice question) in one \\\"\\\\boxed{}\\\"\",\n",
    "                        },\n",
    "                        \n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "            # temperature=0,\n",
    "        )\n",
    "        \n",
    "        answer_dict[qid] = {\"response\":response.choices[0].message.content , \"pred\": extract_answer_from_model_response(response.choices[0].message.content)}\n",
    "        if len(answer_dict)%10==0:\n",
    "            np.save(\"perspective_gpt4o.npy\", answer_dict)\n",
    "np.save(\"perspective_gpt4o.npy\", answer_dict)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4c0f5-cfc9-44fc-84ed-086f3f6f8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import anthropic\n",
    "import json\n",
    "client = anthropic.Anthropic()\n",
    "import base64\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "def encode_image(pil_image):\n",
    "    pil_image.save(\"persclaude.png\")\n",
    "    image_path = \"persclaude.png\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    return image_data\n",
    "\n",
    "def extract_answer_from_model_response(model_response):\n",
    "    model_answer = model_response.strip()\n",
    "    model_answer = model_answer.replace(\"option\", \"\")\n",
    "    # model_answer = model_answer.replace(\"textbf\", \"text\")\n",
    "    \n",
    "    model_answer = model_answer.split('oxed{')[-1][0]\n",
    "        \n",
    "    # model_answer = find_math_answer(model_answer).replace('(a)', 'a').replace('(b)', 'b').replace('(c)', 'c').replace('(d)', 'd').replace('(e)', 'e').replace('{a}', 'a').replace('{b}', 'b').replace('{c}', 'c').replace('{d}', 'd').replace('{e}', 'e').replace('*', '').rstrip('.').lstrip(':').strip()\n",
    "    return model_answer\n",
    "\n",
    "answer_dict= {}\n",
    "image_media_type = \"image/png\"\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(f\"MahtabBg/NEWPerspective\")\n",
    "dataset = dataset['train']\n",
    "if True:\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        example = dataset[i]\n",
    "        qid = example['id']\n",
    "\n",
    "        \n",
    "       \n",
    "        question_image = example['topdown']\n",
    "        choice_image = example['choices']\n",
    "      \n",
    "        \n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=1024,\n",
    "            # temperature =0,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 1>:\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": image_media_type,\n",
    "                                \"data\": encode_image(question_image),\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 2>:\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": image_media_type,\n",
    "                                \"data\": encode_image(choice_image),\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"<Image 1> shows an image from the top of a scene with a red square indicating an agent and a red arrow indicating the agent's direction of view.\\nSelect from the <Image 2> which image represents the agent's view. Please first solve the problem step by step, then put your final answer or a single letter (if it is a multiple choice question) in one \\\"\\\\boxed{}\\\"\"\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        answer_dict[qid] = { \"response\":message.content[0].text, \"pred\": extract_answer_from_model_response(message.content[0].text)}\n",
    "        if len(answer_dict)%10==0:\n",
    "            np.save(\"perspective_claude_sonnet.npy\", answer_dict)\n",
    "            \n",
    "        \n",
    "np.save(\"perspective_claude_sonnet.npy\", answer_dict)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6bd239-24b4-441d-b596-44dc978e385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from google import genai\n",
    "import matplotlib.pyplot as plt\n",
    "from google.genai import types\n",
    "\n",
    "# import google.generativeai as genai\n",
    "GEMINI_API_KEY = \"AIzaSyCxuedQMQWQkYSu67h3L5PMXDg3cDmeeBQ\"\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "\n",
    "def gemini_call_single(query, model='gemini-2.0-flash-thinking-exp-01-21', max_tokens=2048):\n",
    "    # import cv2\n",
    "    import requests\n",
    "    import time\n",
    "    import json\n",
    "\n",
    "    # print(query)\n",
    "\n",
    "    while True:\n",
    "        # print('\\n(trying...) ->', )\n",
    "        try:\n",
    "            \n",
    "            content = client.models.generate_content(\n",
    "                model=model,\n",
    "                contents=query,\n",
    "                # config = types.GenerateContentConfig(\n",
    "                #     temperature=0\n",
    "                # )\n",
    "            )\n",
    "            \n",
    "\n",
    "        except Exception as e_msg:\n",
    "            content = '[ERROR] ' + str(e_msg)\n",
    " \n",
    "        if isinstance(content, str):\n",
    "            \n",
    "            content = '[ERROR] ' + content.lower()\n",
    "            print(content)\n",
    "            if 'exceeded call rate limit' in content or 'exhausted' in content:\n",
    "                # retry for unacceptable response\n",
    "                print('\\n(retry later in 5 seconds...) ->')\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            else:\n",
    "                print('\\n(retry later...) ->', content)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        \n",
    "    ########################################\n",
    "    \n",
    "    # print(responseJson[\"choices\"][0][\"message\"][\"content\"])\n",
    "    return content.text\n",
    "\n",
    "\n",
    "def extract_answer_from_model_response(model_response):\n",
    "    model_answer = model_response.strip()\n",
    "    model_answer = model_answer.replace(\"option\", \"\")\n",
    "    # model_answer = model_answer.replace(\"textbf\", \"text\")\n",
    "    \n",
    "    model_answer = model_answer.split('oxed{')[-1][0]\n",
    "        \n",
    "    # model_answer = find_math_answer(model_answer).replace('(a)', 'a').replace('(b)', 'b').replace('(c)', 'c').replace('(d)', 'd').replace('(e)', 'e').replace('{a}', 'a').replace('{b}', 'b').replace('{c}', 'c').replace('{d}', 'd').replace('{e}', 'e').replace('*', '').rstrip('.').lstrip(':').strip()\n",
    "    return model_answer\n",
    "\n",
    "\n",
    "# convert PIL image to base64\n",
    "def pil_to_base64(pil_image):\n",
    "    import io\n",
    "    import base64\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_encoded_str = base64.b64encode(img_byte_arr.getvalue()).decode('ascii')\n",
    "    return img_encoded_str\n",
    "\n",
    "\n",
    "def test_gemini( ):\n",
    "    # load hf dataset\n",
    "    answer_dict = {}\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(f\"MahtabBg/NEWPerspective\")\n",
    "    dataset = dataset['train']\n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "    acc, total = 0,0\n",
    "    from tqdm.notebook import tqdm\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        example = dataset[i]\n",
    "        qid = example['id']\n",
    "       \n",
    "        question_image = example['topdown']\n",
    "        choice_image = example['choices']\n",
    "        query = [\n",
    "            \"<Image 1>:\",\n",
    "            question_image,\n",
    "            \"<Image 2>:\",\n",
    "            choice_image,\n",
    "             \"<Image 1> shows an image from the top of a scene with a red square indicating an agent and a red arrow indicating the agent's direction of view.\\nSelect from the <Image 2> which image represents the agent's view.\",\n",
    "            \"\\nPlease first solve the problem step by step, then put your final answer or a single letter (if it is a multiple choice question) in one \\\"\\\\boxed{}\\\"\"\n",
    "        ]\n",
    "\n",
    "        response = gemini_call_single(query)\n",
    "\n",
    "        pred = extract_answer_from_model_response(response)\n",
    "\n",
    "        answer_dict[qid] = {\"response\": response, \"pred\": pred}\n",
    "        if len(answer_dict)%10==0:\n",
    "            np.save(\"perspective_gemini_flash_2_thinking.npy\", answer_dict)\n",
    "    return answer_dict\n",
    "    \n",
    "\n",
    "answer_dict = test_gemini()\n",
    "np.save(\"perspective_gemini_flash_2_thinking.npy\", answer_dict)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
