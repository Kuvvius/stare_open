{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "\n",
    "proxy_address = \"http://songmingyang:dSpydxsxxhKix63HfIFhjwnZLEInXEDawSoMD35G1IT2CygKnHsJqG9ZHbEP@10.1.20.50:23128/\"\n",
    "os.environ[\"http_proxy\"] = proxy_address\n",
    "os.environ[\"https_proxy\"] = proxy_address\n",
    "os.environ[\"HTTP_PROXY\"] = proxy_address\n",
    "\n",
    "os.environ[\"HTTPS_PROXY\"] = proxy_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_folding_nets_f1(model=\"gpt-4o\", task=\"folding_nets_test\", \n",
    "                         res_dir=\"/mnt/petrelfs/songmingyang/code/reasoning/others/stare_open/vissim_eval/scripts/results/close_source/evals2\",\n",
    "                         dataset_cache_dir = \"/mnt/petrelfs/songmingyang/songmingyang/data/mm/reasoning/vissim\"\n",
    "                         ):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import f1_score\n",
    "    from collections import defaultdict\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    dataset = f\"VisSim/{task}\"\n",
    "    gt_data = load_dataset(dataset, cache_dir=dataset_cache_dir)[\"train\"]\n",
    "\n",
    "    def calc_f1(preds, gts):\n",
    "        preds = [p.lower() for p in preds]\n",
    "        gts = [g.lower() for g in gts]\n",
    "        return f1_score(gts, preds, average='weighted')\n",
    "    \n",
    "\n",
    "    def get_metrics(data, gt_data):\n",
    "        pred_by_type = defaultdict(list)\n",
    "        gt_by_type = defaultdict(list)\n",
    "\n",
    "        # if \"tangram_puzzle\" in task:\n",
    "        # load gt and get the number of steps\n",
    "        qid2steps = {}\n",
    "        for sample in gt_data:\n",
    "            qid = sample['qid']\n",
    "            if \"folding_nets\" in task:\n",
    "                steps = 5\n",
    "            else:\n",
    "                steps = len(json.loads(sample[\"question_info\"])[\"instructions\"])\n",
    "            qid2steps[qid] = steps\n",
    "        pred_by_steps = defaultdict(list)\n",
    "        gt_by_steps = defaultdict(list)\n",
    "\n",
    "        metrics = {}\n",
    "        overall_gt = []\n",
    "        overall_pred = []\n",
    "        for k, v in data.items():\n",
    "            pred = v['pred']\n",
    "            gt_ans = v['gt_ans']\n",
    "            if \"folding_nets\" in task:\n",
    "                variant = \" \".join(k.split(\"_\")[1:])\n",
    "                if \"all vis\" in variant:\n",
    "                    variant = f\"all for valid {gt_ans}\"\n",
    "            elif \"tangram_puzzle\" in task:\n",
    "                variant = \" \".join(k.split(\"_\")[3:])\n",
    "                if \"all for valid\" in variant:\n",
    "                    variant = f\"all for valid {gt_ans}\"\n",
    "            if len(variant.strip()) == 0:\n",
    "                return {}\n",
    "            \n",
    "            \n",
    "            \n",
    "            s_ = qid2steps[k]\n",
    "            pred_by_steps[s_].append(pred.lower())\n",
    "            gt_by_steps[s_].append(gt_ans.lower())\n",
    "            pred_by_type[variant].append(pred.lower())\n",
    "            gt_by_type[variant].append(gt_ans.lower())\n",
    "            overall_gt.append(gt_ans.lower())\n",
    "            overall_pred.append(pred.lower())\n",
    "        metrics[\"overall_f1\"] = calc_f1(overall_pred, overall_gt)\n",
    "        for k in pred_by_type.keys():\n",
    "            metrics[f\"{k}_f1\"] = calc_f1(pred_by_type[k], gt_by_type[k])\n",
    "        for k in pred_by_steps.keys():\n",
    "            metrics[f\"{k}_f1\"] = calc_f1(pred_by_steps[k], gt_by_steps[k])\n",
    "            metrics[f\"{k}_count\"] = len(gt_by_steps[k])\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    if \"vissim\" in task:\n",
    "        data = np.load(f\"{res_dir}/{model}_{task}_0.json.npy\", allow_pickle=True)\n",
    "        try:\n",
    "            d_ = data[0]\n",
    "            new_data = {d_['qid']: d_ for d_ in data}\n",
    "            data = new_data\n",
    "        except:\n",
    "            data = data.item()\n",
    "        metrics = get_metrics(data, gt_data)\n",
    "    else:\n",
    "        metrics = defaultdict(float)\n",
    "        valid_samples = 0\n",
    "        for seed in range(3):\n",
    "            file_path = f\"{res_dir}/{model}_{task}_{seed}.json.npy\"\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                continue\n",
    "            data  = np.load(file_path, allow_pickle=True)\n",
    "            # convert numpy object to dict\n",
    "            try:\n",
    "                d_ = data[0]\n",
    "                new_data = {d_['qid']: d_ for d_ in data}\n",
    "                data = new_data\n",
    "            except:\n",
    "                data = data.item()\n",
    "            metrics_ = get_metrics(data, gt_data)\n",
    "            if len(metrics_) == 0:\n",
    "                print(f\"Empty metrics for {model}_{task}_{seed}.json\")\n",
    "            for k, v in metrics_.items():\n",
    "                metrics[k] += v\n",
    "            valid_samples += 1\n",
    "        for k in metrics.keys():\n",
    "            metrics[k] /= valid_samples\n",
    "    for k in metrics.keys():\n",
    "        print(f\"{k}: {metrics[k]}\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calc_tranformation_acc(model=\"gpt-4o\", task=\"2d\", visim=False, debug=False, \n",
    "                           res_dir=\"/mnt/petrelfs/songmingyang/code/reasoning/others/stare_open/vissim_eval/scripts/results/close_source/evals2\",\n",
    "                           dataset_cache_dir = \"/mnt/petrelfs/songmingyang/songmingyang/data/mm/reasoning/vissim\"):\n",
    "    import numpy as np\n",
    "    from collections import defaultdict\n",
    "    from datasets import load_dataset\n",
    "    task_type = [\"va\", \"text_instruct\"]\n",
    "    # task_type = [\"va\"]\n",
    "    \n",
    "    def get_metrics(data, gt_data):\n",
    "        qid2transformation = {}\n",
    "        for sample in gt_data:\n",
    "            qid = sample['qid']\n",
    "            transformations = sample['transformations']\n",
    "            trans = []\n",
    "            if 'shear' in transformations:\n",
    "                trans.append('shear')\n",
    "            if 'scale' in transformations:\n",
    "                trans.append('scale')\n",
    "            if 'rotate' in transformations:\n",
    "                trans.append('rotate')\n",
    "            if 'translate' in transformations:\n",
    "                trans.append('translate')\n",
    "            if 'flip' in transformations:\n",
    "                trans.append('flip')\n",
    "            # else:\n",
    "            #     print(f\"Unknown transformation for {transformations}\")\n",
    "            qid2transformation[qid] = trans\n",
    "        pred_by_type = defaultdict(list)\n",
    "        gt_by_type = defaultdict(list)\n",
    "\n",
    "        pred_by_steps = defaultdict(list)\n",
    "        gt_by_steps = defaultdict(list)\n",
    "\n",
    "        pred_by_difficulty = defaultdict(list)\n",
    "        gt_by_difficulty = defaultdict(list)\n",
    "\n",
    "        pred_by_transformation = defaultdict(list)\n",
    "        gt_by_transformation = defaultdict(list)\n",
    "\n",
    "        metrics = {}\n",
    "        overall_gt = []\n",
    "        overall_pred = []\n",
    "        valid_samples = 0\n",
    "        for k, v in data.items():\n",
    "            pred = v['pred'].lower()\n",
    "            gt_ans = v['gt_ans'].lower()\n",
    "            variant = \" \".join(k.split(\"_\")[2:-1])\n",
    "            difficulty = k.split(\"_\")[1]\n",
    "            num_steps = k.split(\"_\")[0]\n",
    "            if k not in qid2transformation:\n",
    "                print(f\"qid {k} not in gt_data\")\n",
    "                continue\n",
    "            valid_samples += 1\n",
    "            trans = qid2transformation[k]\n",
    "            # if len(trans) == 1:\n",
    "            pred_by_type[variant].append(pred.lower())\n",
    "            gt_by_type[variant].append(gt_ans.lower())\n",
    "        \n",
    "            if num_steps == \"\" or variant == \"\" or difficulty == \"\":\n",
    "                print(f\"Empty keys for {k}\")\n",
    "                return {}\n",
    "\n",
    "            overall_gt.append(gt_ans.lower())\n",
    "            overall_pred.append(pred.lower())\n",
    "\n",
    "            if visim and variant != \"all\":\n",
    "                continue\n",
    "            pred_by_difficulty[difficulty].append(pred.lower())\n",
    "            gt_by_difficulty[difficulty].append(gt_ans.lower())\n",
    "            pred_by_steps[num_steps].append(pred.lower())\n",
    "            gt_by_steps[num_steps].append(gt_ans.lower())\n",
    "            for t_ in trans:\n",
    "                pred_by_transformation[t_].append(pred.lower())\n",
    "                gt_by_transformation[t_].append(gt_ans.lower())\n",
    "            if debug and pred != gt_ans:\n",
    "                import ipdb\n",
    "                ipdb.set_trace()\n",
    "        if len(overall_gt) == 0:\n",
    "            print(f\"Empty overall_gt\")\n",
    "            return {}\n",
    "        metrics[\"overall_acc\"] = sum([p == g for p, g in zip(overall_pred, overall_gt)])/len(overall_gt)\n",
    "        for k in pred_by_type.keys():\n",
    "            metrics[f\"{k}_acc\"] = sum([p == g for p, g in zip(pred_by_type[k], gt_by_type[k])])/len(gt_by_type[k])\n",
    "        for k in pred_by_difficulty.keys():\n",
    "            metrics[f\"{k}_acc\"] = sum([p == g for p, g in zip(pred_by_difficulty[k], gt_by_difficulty[k])])/len(gt_by_difficulty[k])\n",
    "        for k in pred_by_steps.keys():\n",
    "            metrics[f\"{k}_acc\"] = sum([p == g for p, g in zip(pred_by_steps[k], gt_by_steps[k])])/len(gt_by_steps[k])\n",
    "        for k in pred_by_transformation.keys():\n",
    "            metrics[f\"{k}_acc\"] = sum([p == g for p, g in zip(pred_by_transformation[k], gt_by_transformation[k])])/len(gt_by_transformation[k])\n",
    "        return metrics\n",
    "    if not visim:\n",
    "        gt_data = {}\n",
    "        for task_ in task_type:\n",
    "            dataset = f\"VisSim/{task}_{task_}_test\"\n",
    "            gt_data[task_] = load_dataset(dataset, cache_dir=dataset_cache_dir)[\"train\"]\n",
    "        overall_metrics = defaultdict(float)\n",
    "        valid_sample = 0\n",
    "        for task_ in task_type:\n",
    "            metrics = defaultdict(float)\n",
    "            task_valid_sample = 0\n",
    "            for seed in range(3):\n",
    "                file_path = f\"{res_dir}/{model}_{task}_{task_}_test_{seed}.json.npy\"\n",
    "                if not os.path.exists(file_path):\n",
    "                    print(f\"File not found: {file_path}\")\n",
    "                    continue\n",
    "                data = np.load(file_path, allow_pickle=True)\n",
    "                try:\n",
    "                    d_ = data[0]\n",
    "                    new_data = {d_['qid']: d_ for d_ in data}\n",
    "                    data = new_data\n",
    "                except:\n",
    "                    data = data.item()\n",
    "                metrics_ = get_metrics(data, gt_data[task_])\n",
    "                if len(metrics_) == 0:\n",
    "                    print(f\"Empty metrics for {model}_{task}_{task_}_test_{seed}.json\")\n",
    "                    continue\n",
    "                for k, v in metrics_.items():\n",
    "                    metrics[k] += v\n",
    "                print(f\">>>>>>>>>>{task_} metrics:\")\n",
    "                for k, v in metrics_.items():\n",
    "                    print(f\"\\t\\t{k}: {v}\")\n",
    "                valid_sample += 1\n",
    "                task_valid_sample += 1\n",
    "            print(f\">>>>>>>>>>Task metrics for {task_}:\")\n",
    "            for k in metrics.keys():\n",
    "                metrics[k] /= task_valid_sample\n",
    "                print(f\"{k}: {metrics[k]}\") \n",
    "            for k, v in metrics.items():\n",
    "                overall_metrics[k] += v\n",
    "        print(f\">>>>>>>>>>Overall metrics:\")\n",
    "        for k in overall_metrics.keys():\n",
    "            overall_metrics[k] /= len(task_type)\n",
    "            print(f\"{k}: {overall_metrics[k]}\")\n",
    "    else:\n",
    "\n",
    "        gt_data = {}\n",
    "        for task_ in task_type:\n",
    "            dataset = f\"VisSim/{task}_{task_}_vissim_test\"\n",
    "            gt_data[task_] = load_dataset(dataset, cache_dir=dataset_cache_dir)[\"train\"]\n",
    "        metrics = defaultdict(float)\n",
    "        for task_ in task_type:\n",
    "            file_path = f\"{res_dir}/{model}_{task}_{task_}_vissim_test_0.json.npy\"\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                continue\n",
    "            data = np.load(file_path, allow_pickle=True)\n",
    "            try:\n",
    "                d_ = data[0]\n",
    "                new_data = {d_['qid']: d_ for d_ in data}\n",
    "                data = new_data\n",
    "            except:\n",
    "                data = data.item()\n",
    "            metrics_ = get_metrics(data, gt_data[task_])\n",
    "            for k, v in metrics_.items():\n",
    "                metrics[k] += v\n",
    "            print(f\">>>>>>>>>>{task_} metrics:\")\n",
    "            for k, v in metrics_.items():\n",
    "                print(f\"\\t\\t{k}: {v}\")\n",
    "        print(f\">>>>>>>>>>Overall metrics:\")\n",
    "        for k in metrics.keys():\n",
    "            if k not in [\"all w ans_acc\", \"all last_acc\"]:\n",
    "                metrics[k] /= 2\n",
    "            print(f\"{k}: {metrics[k]}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall_f1: 0.5027343623164179\n",
      "q only_f1: 0.5015633006964447\n",
      "q+steps_f1: 0.5043618816945162\n",
      "5_f1: 0.5027343623164179\n",
      "5_count: 193.0\n"
     ]
    }
   ],
   "source": [
    "# def calc_folding_nets_f1(model=\"gpt-4o\", task=\"folding_nets_test\", \n",
    "#                          res_dir=\"/mnt/petrelfs/songmingyang/code/reasoning/others/stare_open/vissim_eval/scripts/results/close_source/evals2\",\n",
    "#                          dataset_cache_dir = \"/mnt/petrelfs/songmingyang/songmingyang/data/mm/reasoning/vissim\"\n",
    "#                          ):\n",
    "calc_folding_nets_f1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>va metrics:\n",
      "\t\toverall_acc: 0.7647058823529411\n",
      "\t\tall_acc: 0.7598039215686274\n",
      "\t\tpartial_acc: 0.7745098039215687\n",
      "\t\teasy_acc: 0.8088235294117647\n",
      "\t\tmedium_acc: 0.7941176470588235\n",
      "\t\thard_acc: 0.6764705882352942\n",
      "\t\t2steps_acc: 0.7843137254901961\n",
      "\t\t3steps_acc: 0.7352941176470589\n",
      "\t\trotate_acc: 0.7281553398058253\n",
      "\t\ttranslate_acc: 0.8\n",
      "\t\tshear_acc: 0.5476190476190477\n",
      "\t\tscale_acc: 0.9193548387096774\n",
      "\t\tflip_acc: 0.7692307692307693\n",
      ">>>>>>>>>>text_instruct metrics:\n",
      "\t\toverall_acc: 0.8809815950920246\n",
      "\t\tall w ans_acc: 0.8690095846645367\n",
      "\t\tall_acc: 0.8939393939393939\n",
      "\t\tall last_acc: 0.8939393939393939\n",
      "\t\tpartial_acc: 0.8679245283018868\n",
      "\t\teasy_acc: 0.9154929577464789\n",
      "\t\tmedium_acc: 0.8939393939393939\n",
      "\t\thard_acc: 0.8688524590163934\n",
      "\t\t2steps_acc: 0.8804347826086957\n",
      "\t\t3steps_acc: 0.9056603773584906\n",
      "\t\trotate_acc: 0.8598130841121495\n",
      "\t\ttranslate_acc: 0.9151515151515152\n",
      "\t\tflip_acc: 0.9117647058823529\n",
      "\t\tscale_acc: 0.9324324324324325\n",
      ">>>>>>>>>>Overall metrics:\n",
      "overall_acc: 0.8228437387224828\n",
      "all_acc: 0.8268716577540107\n",
      "partial_acc: 0.8212171661117278\n",
      "easy_acc: 0.8621582435791217\n",
      "medium_acc: 0.8440285204991087\n",
      "hard_acc: 0.7726615236258438\n",
      "2steps_acc: 0.8323742540494459\n",
      "3steps_acc: 0.8204772475027747\n",
      "rotate_acc: 0.7939842119589874\n",
      "translate_acc: 0.8575757575757577\n",
      "shear_acc: 0.27380952380952384\n",
      "scale_acc: 0.925893635571055\n",
      "flip_acc: 0.8404977375565611\n",
      "all w ans_acc: 0.8690095846645367\n",
      "all last_acc: 0.8939393939393939\n"
     ]
    }
   ],
   "source": [
    "# def calc_tranformation_acc(model=\"gpt-4o\", task=\"2d\", visim=False, debug=False, \n",
    "#                            res_dir=\"/mnt/petrelfs/songmingyang/code/reasoning/others/stare_open/vissim_eval/scripts/results/close_source/evals2\",\n",
    "#                            dataset_cache_dir = \"/mnt/petrelfs/songmingyang/songmingyang/data/mm/reasoning/vissim\"):\n",
    "# calc_tranformation_acc()\n",
    "# calc_tranformation_acc(\"claude-3-5-sonnet-20241022\", \"2d\", visim=False, debug=False)\n",
    "# calc_tranformation_acc(\"gemini-2.0-flash-thinking-exp-01-21\", \"2d\", visim=False, debug=False)\n",
    "# metrics = calc_tranformation_acc(visim=True)\n",
    "calc_tranformation_acc(\"claude-3-5-sonnet-20241022\", \"2d\", visim=True, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r1-v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
